[{"path":"https://tpetric7.github.io/bertopicr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Teodor Petrič Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/articles/load_and_reuse_model.html","id":"load-r-packages","dir":"Articles","previous_headings":"","what":"Load R packages","title":"Load and Reuse a BERTopic Model","text":"Python environment selection checks handled hidden setup chunk top vignette.","code":"library(reticulate) library(bertopicr) library(readr) library(dplyr)"},{"path":"https://tpetric7.github.io/bertopicr/articles/load_and_reuse_model.html","id":"gpu-availability-optional","dir":"Articles","previous_headings":"","what":"GPU availability (optional)","title":"Load and Reuse a BERTopic Model","text":"","code":"reticulate::py_run_string(code = \"import torch print(torch.cuda.is_available())\") # if GPU is available then TRUE else FALSE"},{"path":"https://tpetric7.github.io/bertopicr/articles/load_and_reuse_model.html","id":"load-the-model-bundle","dir":"Articles","previous_headings":"","what":"Load the model bundle","title":"Load and Reuse a BERTopic Model","text":"","code":"loaded <- load_bertopic_model(\"topic_model\") # set the location of the model! model <- loaded$model extras <- loaded$extras"},{"path":"https://tpetric7.github.io/bertopicr/articles/load_and_reuse_model.html","id":"load-data-for-inspection","dir":"Articles","previous_headings":"","what":"Load data for inspection","title":"Load and Reuse a BERTopic Model","text":"","code":"sample_path <- system.file(\"extdata\", \"spiegel_sample.rds\", package = \"bertopicr\") df <- read_rds(sample_path) docs <- df |> pull(text_clean)"},{"path":"https://tpetric7.github.io/bertopicr/articles/load_and_reuse_model.html","id":"create-tables-from-the-loaded-model","dir":"Articles","previous_headings":"","what":"Create tables from the loaded model","title":"Load and Reuse a BERTopic Model","text":"","code":"doc_info <- get_document_info_df(model = model, texts = docs) topic_info <- get_topic_info_df(model = model) topics_df <- get_topics_df(model = model)"},{"path":"https://tpetric7.github.io/bertopicr/articles/load_and_reuse_model.html","id":"use-extras-and-visualizations","dir":"Articles","previous_headings":"","what":"Use extras and visualizations","title":"Load and Reuse a BERTopic Model","text":"following visualizations work topics_over_time topics_per_class defined model training within train_bertopic_model() function.","code":"visualize_barchart(model = model, filename = \"barchart_demo\") visualize_distribution(   model = model,   text_id = 1,   probabilities = extras$probabilities,   filename = \"vis_topic_dist_demo\" ) visualize_heatmap(model = model, filename = \"vis_heat_demo\") visualize_topics(model = model, filename = \"dist_map_demo\") visualize_documents(model = model, docs, reduced_embeddings = extras$reduced_embeddings_2d) visualize_documents_2d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_2d) visualize_documents_3d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_3d) visualize_topics_over_time(model = model, topics_over_time_model = extras$topics_over_time) visualize_topics_per_class(model, extras$topics_per_class, auto_open = FALSE)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"load-r-packages","dir":"Articles","previous_headings":"","what":"Load R packages","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"Python environment selection checks handled hidden setup chunk top vignette. Load R packages initialize Python environment reticulate package. default, reticulate uses isolated Python virtual environment named r-reticulate (cf. https://rstudio.github.io/reticulate/). use_python() use_virtualenv() functions enable specify alternate Python environment (cf. https://rstudio.github.io/reticulate/). Note: Avoid loading conflicting R libraries (like arrow) alongside Python modules BERTopic, pyarrow plotly. Note: prefer streamlined setup, setup_python_environment() (see README) can install Python dependencies. helper functions train_bertopic_model(), save_bertopic_model(), load_bertopic_model() showcased train_and_save_model.Rmd load_and_reuse_model.Rmd.","code":"library(dplyr) library(tidyr) library(purrr) library(utils) library(tibble) library(readr) library(tictoc) library(htmltools) library(bertopicr)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"python-packages","dir":"Articles","previous_headings":"","what":"Python packages","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"Use bertopicr::setup_python_environment() (see README) install required Python packages configure environment. hidden setup chunk top vignette checks availability skip Python chunks environment ready. Install ollama lm-studio want serve local language models. setup, import Python packages used topic modeling .","code":"# Import necessary Python modules py <- import_builtins() np <- import(\"numpy\") umap <- import(\"umap\") UMAP <- umap$UMAP hdbscan <- import(\"hdbscan\") HDBSCAN <- hdbscan$HDBSCAN sklearn <- import(\"sklearn\") CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer bertopic <- import(\"bertopic\") plotly <- import(\"plotly\") datetime <- import(\"datetime\")"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"text-preparation","dir":"Articles","previous_headings":"","what":"Text preparation","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"German texts text_clean column. segmented smaller chunks (100 tokens long) optimize topic extraction bertopicr. Special characters removed cleaning function. text chunks lower case. collected stopword list includes German English tokens inserted Python’s CountVectorizer c-TF-IDF calculation. embedding model process text chunks text_clean column stopword removal. lists texts_cleaned timesteps, need model preparation, topic extraction visualization.","code":"rds_path <- file.path(\"inst/extdata\", \"spiegel_sample.rds\") dataset <- read_rds(rds_path) names(dataset) dim(dataset) stopwords_path <- file.path(\"inst/extdata\", \"all_stopwords.txt\") all_stopwords <- read_lines(stopwords_path) texts_cleaned = dataset$text_clean titles = dataset$doc_id timestamps <- as.list(dataset$date) # timestamps <- as.integer(dataset$year)  texts_cleaned[[1]]"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"model-preparation","dir":"Articles","previous_headings":"","what":"Model Preparation","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"model preparation, going use reticulate interface Python modules. R code essentially conversion Python code. Topic model preparation also include local language models (via ollama lm-studio) leveraging OpenAI endpoint.","code":""},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"embeddings","dir":"Articles","previous_headings":"Model Preparation","what":"Embeddings","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"SentenceTransformer creates necessary embeddings (vector representations text tokens) topic modeling bertopic. first time SentenceTransformer used specific model, model downloaded huggingface website (https://huggingface.co/), many freely usable language models hosted (https://huggingface.co/models).","code":"# Embed the sentences sentence_transformers <- import(\"sentence_transformers\") SentenceTransformer <- sentence_transformers$SentenceTransformer # choose an appropriate embeddings model embedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\") embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"dimension-reduction","dir":"Articles","previous_headings":"Model Preparation","what":"Dimension reduction","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"next two steps, umap module reduces number dimensions embeddings, hdbscan module extracts clusters can evaluated topic pipeline. dimension reduction methods (like PCA tSNE) can used instead.","code":"# Initialize UMAP and HDBSCAN models umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"clustering","dir":"Articles","previous_headings":"Model Preparation","what":"Clustering","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"hdbscan module extracts clusters can evaluated topic pipeline. Starting BERTopic version 0.17.0, decrease number parallel workers core_dist_n_jobs = 1 avoid memory error messages Windows PC. clustering methods (like kmeans) can used instead.","code":"hdbscan_model <- HDBSCAN(min_cluster_size=50L, min_samples = 20L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE, core_dist_n_jobs = 1)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"c-tf-idf","dir":"Articles","previous_headings":"Model Preparation","what":"c-TF-IDF","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"Countvectorizer calculates c-TF-IDF frequencies enables representation model defined extract suitable keywords descriptors extracted topics. Stopwords removed embeddings creation, keyword extraction. Stopword removal accomplished CountVectorizer method.","code":"# Initialize CountVectorizer vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),                                      max_features = 10000L, max_df = 50L,                                     stop_words = all_stopwords) sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned) sentence_vectors_dense <- np$array(sentence_vectors) sentence_vectors_dense <- py_to_r(sentence_vectors_dense)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"representation-models","dir":"Articles","previous_headings":"Model Preparation","what":"Representation models","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"example , multiple representation models used keyword extraction identified topics topic description: keyBERT (part BERTopic), language model (served locally ollama via OpenAI endpoint, also possible use models Groq providers), Maximal Marginal Relevance model (MMR) spacy POS representation model. default, one representation model created bertopic. running following chunk, make sure representation models downloaded installed set BERTOPICR_ENABLE_REPR=true. Otherwise, comment representation models missing won’t used topic training pipeline. want minimal workflow, use train_bertopic_model() instead (see train_and_save_model.Rmd). Note: use train_bertopic_model() helper (see Quick Start section Readme.md vignette load_and_reuse.Rmd) instead procedure notebook, can include one representation model (default = “none”). prompt describes task language model accomplish, mentions documents work topic labels derive text contents keywords.","code":"# Initialize representation models keybert_model <- bertopic$representation$KeyBERTInspired() openai <- import(\"openai\") OpenAI <- openai$OpenAI ollama <- import(\"ollama\") # lmstudio <- import(\"lmstudio\")  # Point to the local server (ollama or lm-studio) client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama') # client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')  prompt <- \" I have a topic that contains the following documents: [DOCUMENTS] The topic is described by the following keywords: [KEYWORDS]  Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format: topic: <topic label> \"  # download an appropriate LLM to be hosted by ollama or lm-studio openai_model <- bertopic$representation$OpenAI(client,                                                 model = \"gpt-oss:20b\",                                                 exponential_backoff = TRUE,                                                 chat = TRUE,                                                 prompt = prompt)  # downlaod a language model from spacy.io before use here # Below a German spacy model is used pos_model <- bertopic$representation$PartOfSpeech(\"de_core_news_lg\") # diversity set relatively high to reduce repetition of keyword word forms mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)  # Combine all representation models representation_model <- list(   \"KeyBERT\" = keybert_model,   \"OpenAI\" = openai_model,   \"MMR\" = mmr_model,   \"POS\" = pos_model )"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"zeroshot-keywords","dir":"Articles","previous_headings":"Model Preparation","what":"Zeroshot keywords","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"Bertopic enables us define zeroshot list keywords can used drive topic model towards desired topic outcomes. topic model , zeroshot keyword list disabled, can activated needed.","code":"# We can define a number of topics of interest  zeroshot_topic_list  <- list(\"german national identity\", \"minority issues in germany\")"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-model","dir":"Articles","previous_headings":"Model Preparation","what":"Topic Model","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"next step, initialize BERTopic model pipeline hyperparameters.","code":"# Initialize BERTopic model with pipeline models and hyperparameters BERTopic <- bertopic$BERTopic topic_model <- BERTopic(   embedding_model = embedding_model,   umap_model = umap_model,   hdbscan_model = hdbscan_model,   vectorizer_model = vectorizer_model,   # zeroshot_topic_list = zeroshot_topic_list,   # zeroshot_min_similarity = 0.85,   representation_model = representation_model,   calculate_probabilities = TRUE,   top_n_words = 200L, # if you need more top words, insert the desired number here!!!   verbose = TRUE )"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"model-training","dir":"Articles","previous_headings":"Model Preparation","what":"Model Training","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"preparational steps, topic model ready trained : topic_model$fit_transform(texts, embeddings). obtain topic labels topics <- fit_transform[[1]] topic probabilities probs <- fit_transform[[2]].","code":"tictoc::tic()  # Fit the model and transform the texts fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings) topics <- fit_transform[[1]]  # Now transform the texts to get the updated probabilities transform_result <- topic_model$transform(texts_cleaned) probs <- transform_result[[2]]  # Extract the updated probabilities  tictoc::toc()"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-dynamics","dir":"Articles","previous_headings":"Model Preparation","what":"Topic Dynamics","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"Since dataset contains time-related metadata, can use timestamps dynamic topic modeling, .e., discovering topic development topic sequences time. data doesn’t contain time-related column, skip disable timestamps topics_over_time calculations.","code":"# Converting R Date to Python datetime datetime <- import(\"datetime\")  timestamps <- as.list(dataset$date) # timestamps <- as.integer(dataset$year)  # Convert each R date object to an ISO 8601 string timestamps <- lapply(timestamps, function(x) {   format(x, \"%Y-%m-%dT%H:%M:%S\")  # ISO 8601 format })  # Dynamic topic model topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"store-results","dir":"Articles","previous_headings":"Model Preparation","what":"Store Results","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"topic labels probabilities stored dataframe named results, together variables metadata.","code":"# Combine results with additional columns results <- dataset |>    mutate(Topic = topics,           Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence  results <- results |>    mutate(row_id = row_number()) |>    select(row_id, everything())  head(results,10) |> rmarkdown::paged_table() results |>   saveRDS(\"inst/extdata/spiegel_topic_results_df.rds\", version = 2) results |>   write_csv(\"inst/extdata/spiegel_topic_results_df.csv\")"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"results","dir":"Articles","previous_headings":"","what":"Results","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"R package bertopicr used section display topic modeling results form lists, data frames visualizations. names functions nearly Python package BERTopic.","code":""},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"document-information","dir":"Articles","previous_headings":"Results","what":"Document information","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"get_document_info_df() creates dataframe contains documents associated topics, characteristic keywords, probability scores, representative documents topic representation model results (e.g., keywords extracted KeyBERT, MMR, spacy, LLM descriptions topics).","code":"library(bertopicr) document_info_df <- get_document_info_df(model = topic_model,                                           texts = texts_cleaned,                                           drop_expanded_columns = TRUE) document_info_df |> head() |> rmarkdown::paged_table()"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"representative-docs","dir":"Articles","previous_headings":"Results","what":"Representative docs","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"First, create data frame similar df_docs , contains columns Topic, Document probs. use get_most_representative_docs() function extract representative documents chosen topic.","code":"# Create a data frame similar to df_docs df_docs <- tibble(Topic = results$Topic,                   Document = results$text_clean,                   probs = results$Probability) rep_docs <- get_most_representative_docs(df = df_docs,                                           topic_nr = 3,                                           n_docs = 5) unique(rep_docs)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-information","dir":"Articles","previous_headings":"Results","what":"Topic information","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"function get_topic_info_df() creates another useful data frame, extracted topics shows number associated documents (text chunks), topic id (Name), characeristic keywords according chosen representation models three (concatenated) representative documents.","code":"topic_info_df <- get_topic_info_df(model = topic_model,                                     drop_expanded_columns = TRUE) head(topic_info_df) |> rmarkdown::paged_table()"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"words-in-topics","dir":"Articles","previous_headings":"Results","what":"Words in Topics","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"get_topics_df() function concentrates words associated certain topic probability scores. outliers (Topic = -1) usually included analysis. BERTopic offers function reduce number outliers update topic model.","code":"topics_df <- get_topics_df(model = topic_model) head(topics_df, 10)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-barchart","dir":"Articles","previous_headings":"Results","what":"Topic Barchart","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"visualize_barchart() creates interactive barchart top five words frequently occurring topics. might prefer create customizable barchart ggplot2 package using dataframe extracted get_topics_df() function run plotly library R ggplotly() function interactive barchart. Due possible conflicts R’s plotly library Python’s plotly implementation, interactive barchart disabled.","code":"visualize_barchart(model = topic_model,                     filename = \"topics_topwords_interactive_barchart.html\", # default                    open_file = FALSE) # TRUE enables output in browser library(ggplot2)  barchart <- topics_df |>    group_by(Topic) |>    filter(Topic >= 0 & Topic <= 8) |>    slice_head(n=5) |>    mutate(Topic = paste(\"Topic\", as.character(Topic)),           Word = reorder(Word, Score)) |>    ggplot(aes(Score, Word, fill = Topic)) +   geom_col() +   facet_wrap(~ Topic, scales = \"free\") +   theme(legend.position = \"none\")  # # Disabled to avoid poential conflicts # library(plotly) # ggplotly(barchart)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"find-topics","dir":"Articles","previous_headings":"Results","what":"Find Topics","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"find_topics_df() function useful semantic search. can identify topics associated chosen query multiple queries.","code":"find_topics_df(model = topic_model,                 queries = \"migration\", # user input                top_n = 10, # default                return_tibble = TRUE) # default find_topics_df(model = topic_model,                                 queries = c(\"migranten\", \"asylanten\"),                                top_n = 5)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"get-topics","dir":"Articles","previous_headings":"Results","what":"Get Topics","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"get_topic_df() function creates dataframe contains top words extracted chosen topic.","code":"get_topic_df(model = topic_model,                             topic_number = 0,                             top_n = 5, # default is 10                            return_tibble = TRUE) # default"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-distribution","dir":"Articles","previous_headings":"Results","what":"Topic Distribution","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"visualize_distribution() function produces interactive barchart displays associated topics chosen document (text chunk). probability scores help identify likely topic(s) document (text chunk).","code":"# default filename: topic_dist_interactive.html visualize_distribution(model = topic_model,                         text_id = 1, # user input                        probabilities = probs) # see model training"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"intertopic-distance-map","dir":"Articles","previous_headings":"Results","what":"Intertopic Distance Map","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"semantic relatedness distance topics can displayed form map visualize_topics() function.","code":"visualize_topics(model = topic_model,                   filename = \"intertopic_distance_map\") # default name"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-similarity","dir":"Articles","previous_headings":"Results","what":"Topic Similarity","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"can create similarity matrix applying cosine similarities generated topic embeddings. resulting matrix indicates similar topics . visualize similarity matrix can use visualize_heatmap() function.","code":"visualize_heatmap(model = topic_model,                    filename = \"topics_similarity_heatmap\",                    auto_open = FALSE)"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-hierarchy","dir":"Articles","previous_headings":"Results","what":"Topic hierarchy","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"best way display relatedness documents (text chunks) visualize_hierarchy() function, creates interactive dendrogram. additional option creation hierarchical topics list can included interactive dendrogram enables user identify joint expressions.","code":"visualize_hierarchy(model = topic_model,                      hierarchical_topics = NULL, # default                     filename = \"topic_hierarchy\", # default name, html extension                     auto_open = FALSE) # TRUE enables output in browser hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned) visualize_hierarchy(model = topic_model,                      hierarchical_topics = hierarchical_topics,                     filename = \"topic_hierarchy\", # default name, html extension                     auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"visualize-documents","dir":"Articles","previous_headings":"Results","what":"Visualize Documents","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"visualize_documents() function displays identified clusters associated certain topic two dimensions. Usually, best reduce dimensionality embeddings UMAP (another dimension reduction method) produce intelligible visual results. interactive plot allows user select one clusters double-click mouse. updating BERTopic=0.17.0, might experience visualize_documents() function doesn’t render dots scatterplot. simple temporary fix open Python file _documents.py visualize_documents() function ofBERTopic (Windows system sits anaconda3\\envs\\bertopic\\Lib\\site-packages\\bertopic\\plotting\\) change go.Scattergl go.Scatter fig.add_trace() function (occurs twice Python script). visualize_documents_2d() function variant interactive plot , additional tooltips. Set n_components = 2L reduced_embeddings! create interactive 3D plot, visualize_documents_3d() function can used. function implemented Python package. Set n_components = 3L reduced_embeddings! legend updated visualize_documents_3d() function shows Topic keywords (Name) instead Topic number includes tooltips.","code":"# Reduce dimensionality of embeddings using UMAP reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)  visualize_documents(model = topic_model,                      texts = texts_cleaned,                      reduced_embeddings = reduced_embeddings,                      filename = \"visualize_documents\", # default extension html                     auto_open = FALSE) # TRUE enables output in browser # Reduce dimensionality of embeddings using UMAP (n_components = 2L !!!) reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)  visualize_documents_2d(model = topic_model,                         texts = texts_cleaned,                         reduced_embeddings = reduced_embeddings,                         custom_labels = FALSE, # default                        hide_annotation = TRUE, # default                        tooltips = c(\"Topic\", \"Name\", \"Probability\", \"Text\"), # default                        filename = \"visualize_documents_2d\", # default name                        auto_open = FALSE) # TRUE enables output in browser # Reduce dimensionality of embeddings using UMAP reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 3L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)  visualize_documents_3d(model = topic_model,                         texts = texts_cleaned,                         reduced_embeddings = reduced_embeddings,                         custom_labels = FALSE, # default                        hide_annotation = TRUE, # default                        tooltips = c(\"Topic\", \"Name\", \"Probability\", \"Text\"), # default                        filename = \"visualize_documents_3d\", # default name                        auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"topic-development","dir":"Articles","previous_headings":"Results","what":"Topic Development","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"can also inspect chosen number topics develop certain period time. visualize_topics_over_time() function assumes timestamps, topic model topics time model already defined (e.g., model preparation step topic model training). timestamps need integers certain date format (see model preparation step ).","code":"visualize_topics_over_time(model = topic_model,                             # see Topic Dynamics section above                            topics_over_time_model = topics_over_time,                            top_n_topics = 10, # default is 20                            filename = \"topics_over_time\") # default, html extension"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"groups","dir":"Articles","previous_headings":"Results","what":"Groups","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"dataset includes categorical variables (groups, classes, etc.), can use visualize_topics_per_class() function display interactive barchart groups classes associated chosen topic. double-click mouse, user can choose single topic inspect frequency groups. also use ggplot2 plotly packages R produce similar looking customized interactive barchart.","code":"classes = as.list(dataset$genre) # text types topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)  visualize_topics_per_class(model = topic_model,                             topics_per_class = topics_per_class,                            start = 0, # default                            end = 10, # default                            filename = \"topics_per_class\", # default, html extension                             auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"wordcloud","dir":"Articles","previous_headings":"","what":"Wordcloud","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"Wordclouds implemented BERTopic, possible extract top n words topic. First, spin new BERTopic model top_n = 200L (.e., 200 frequent words). create dataframe plot wordcloud.","code":"BERTopic200 <- bertopic$BERTopic topic_model200 <- BERTopic200(   embedding_model = embedding_model,   umap_model = umap_model,   hdbscan_model = hdbscan_model,   vectorizer_model = vectorizer_model,   # zeroshot_topic_list = zeroshot_topic_list,   # zeroshot_min_similarity = 0.85,   representation_model = representation_model,   calculate_probabilities = TRUE,   top_n_words = 200L, # !!!   verbose = TRUE )  tictoc::tic()  # Fit the model and transform the texts py_fit <- topic_model200$fit(texts_cleaned, embeddings)  # ask Python for the top-200 of the desired topic: py_topic200 <- py_fit$get_topic(1L, 200L)    # list of (word, score)  names(py_topic200)  rep_list <- py_topic200[[\"Main\"]]  tictoc::toc() df_wc <- data.frame(   name = sapply(rep_list, `[[`, 1),   freq = as.numeric(sapply(rep_list, `[[`, 2)),   stringsAsFactors = FALSE )  library(wordcloud2) source(\"inst/extdata/wordcloud2a.R\")  wordcloud2a(   data            = df_wc,   size            = 0.5,   minSize         = 0,   gridSize        = 1,   fontFamily      = \"Segoe UI\",   fontWeight      = \"bold\",   color           = \"random-dark\",   backgroundColor = \"white\",   shape           = \"circle\",   ellipticity     = 0.65 )"},{"path":"https://tpetric7.github.io/bertopicr/articles/topics_spiegel.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Topic Modeling with BERTopic in R using reticulate and local LLMs","text":"BERTopic awesome topic modeling package Python. bertopicr package tries bring functionalities R programming environment magnificent reticulate package interface Python backend. BERTopic offers number additional functions, might included subsequent versions bertopicr.","code":""},{"path":"https://tpetric7.github.io/bertopicr/articles/train_and_save_model.html","id":"load-r-packages","dir":"Articles","previous_headings":"","what":"Load R packages","title":"Train and Save a BERTopic Model","text":"Python environment selection checks handled hidden setup chunk top vignette.","code":"library(reticulate) library(bertopicr) library(readr) library(dplyr)"},{"path":"https://tpetric7.github.io/bertopicr/articles/train_and_save_model.html","id":"gpu-availability-optional","dir":"Articles","previous_headings":"","what":"GPU availability (optional)","title":"Train and Save a BERTopic Model","text":"","code":"reticulate::py_run_string(code = \"import torch print(torch.cuda.is_available())\") # if GPU is available then TRUE else FALSE"},{"path":"https://tpetric7.github.io/bertopicr/articles/train_and_save_model.html","id":"load-sample-data","dir":"Articles","previous_headings":"","what":"Load sample data","title":"Train and Save a BERTopic Model","text":", German sample dataframe used topic analysis.","code":"sample_path <- system.file(\"extdata\", \"spiegel_sample.rds\", package = \"bertopicr\") df <- read_rds(sample_path) docs <- df |> pull(text_clean)"},{"path":"https://tpetric7.github.io/bertopicr/articles/train_and_save_model.html","id":"train-the-model","dir":"Articles","previous_headings":"","what":"Train the model","title":"Train and Save a BERTopic Model","text":"train_bertopic_model() function convenience function. options / parameter finetuning, see vignette (topics_spiegel.Rmd) Quarto file (inst/extdata/topics_spiegel.qmd). settings train_bertopic_model() function, check help file.","code":"topic_model <- train_bertopic_model(   docs = docs,   top_n_words = 50L, # set integer numbger of top words   embedding_model = \"Qwen/Qwen3-Embedding-0.6B\", # choose your (multilingual) model from huggingface.co   embedding_show_progress = TRUE,   timestamps = df$date, # set this to NULL if not applicable with your data   classes = df$genre, # set this to NULL if not applicable with your data   representation_model = \"keybert\" # keyword generation for each topic )"},{"path":"https://tpetric7.github.io/bertopicr/articles/train_and_save_model.html","id":"save-the-model-and-extras","dir":"Articles","previous_headings":"","what":"Save the model and extras","title":"Train and Save a BERTopic Model","text":"BERTopic - WARNING: use pickle save/load BERTopic model,please make sure environments save load model exactly . version BERTopic,dependencies, python need remain .","code":"save_bertopic_model(topic_model, \"topic_model\")"},{"path":"https://tpetric7.github.io/bertopicr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Teodor Petrič. Author, maintainer.","code":""},{"path":"https://tpetric7.github.io/bertopicr/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Petrič T (2026). bertopicr: Topic Modeling BERTopic. R package version 0.3.6, https://tpetric7.github.io/bertopicr/.","code":"@Manual{,   title = {bertopicr: Topic Modeling with BERTopic},   author = {Teodor Petrič},   year = {2026},   note = {R package version 0.3.6},   url = {https://tpetric7.github.io/bertopicr/}, }"},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"bertopicr","dir":"","previous_headings":"","what":"Topic Modeling with BERTopic","title":"Topic Modeling with BERTopic","text":"Topic modeling R via reticulate + Python BERTopic ecosystem (version 0.17.x). Provides helpers training, persistence, topic inspection, visualization; see Quarto notebook vignettes end--end workflow.","code":""},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"installation-r-package","dir":"","previous_headings":"","what":"Installation (R package)","title":"Topic Modeling with BERTopic","text":"","code":"install.packages(\"devtools\") devtools::install_github(\"tpetric7/bertopicr\")"},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"python-environment-setup-pick-one","dir":"","previous_headings":"","what":"Python environment setup (pick one)","title":"Topic Modeling with BERTopic","text":". Install inside R via reticulate Requires Python installed discoverable R package reticulate. Install Python python.org restart R Windows. Installation setup_python_environment() function: Alternatively, setup following lines code: B. Virtualenv (base Python) C. Conda (Requirements bundled inst/requirements.txt. GPU, install matching CUDA build PyTorch env, e.g. pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118.)","code":"library(bertopicr) library(reticulate)  setup_python_environment(   envname = \"r-bertopic\",   method = \"virtualenv\" # or \"conda\" )  # Point reticulate at the environment you just created use_virtualenv(\"r-bertopic\", required = TRUE) # or use_condaenv(\"r-bertopic\", required = TRUE) py_config()  # confirm reticulate sees the chosen env library(reticulate) # Choose ONE of these depending on what you created target_env <- \"r-bertopic\" use_virtualenv(target_env, required = TRUE)      # for virtualenv # use_condaenv(target_env, required = TRUE)      # for conda  req <- system.file(\"requirements.txt\", package = \"bertopicr\") # If req is \"\", reinstall/upgrade the package so the file is available. py_install(packages = c(\"-r\", req), envname = target_env, method = \"auto\", pip = TRUE) py_config()  # confirm reticulate sees the chosen env python -m venv r-bertopic  # Windows r-bertopic\\Scripts\\activate  # macOS/Linux source r-bertopic/bin/activate  pip install --upgrade pip pip install -r inst/requirements.txt conda create -n r-bertopic python=3.10 conda activate r-bertopic pip install -r inst/requirements.txt"},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"macos-notes","dir":"","previous_headings":"","what":"macOS notes","title":"Topic Modeling with BERTopic","text":"reticulate fails load Python libraries macOS, install Homebrew zlib set fallback library path per session: can install zlib Homebrew:","code":"bertopicr::configure_macos_homebrew_zlib() brew install zlib"},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"quick-start-fit--visualize","dir":"","previous_headings":"","what":"Quick Start (fit + visualize)","title":"Topic Modeling with BERTopic","text":"package includes helpers setup, training, persistence. can still use BERTopic training code, pass Python model outputs R helpers.","code":"library(reticulate) library(bertopicr)  # Point reticulate to the env you prepared use_virtualenv(\"r-bertopic\", required = TRUE) # use_condaenv(\"r-bertopic\", required = TRUE)  # Example: train in R (use a real sample to avoid tiny-N failures) sample_path <- system.file(\"extdata\", \"spiegel_sample.rds\", package = \"bertopicr\") df <- readr::read_rds(sample_path) texts <- df$text_clean[seq_len(500)] topic_model <- train_bertopic_model(   texts,   embedding_model = \"Qwen/Qwen3-Embedding-0.6B\",   top_n_words = 3L ) # Note: tiny datasets can trigger UMAP spectral warnings/errors; using a # realistic sample size and a smaller top_n_words avoids that. save_bertopic_model(topic_model, \"topic_model\")  loaded <- load_bertopic_model(\"topic_model\") model <- loaded$model probs <- loaded$extras$probabilities  # Use the R helpers visualize_topics(model, filename = \"intertopic_distance_map\", auto_open = FALSE) visualize_distribution(model, text_id = 1, probabilities = probs, auto_open = FALSE)"},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"advanced-example","dir":"","previous_headings":"","what":"Advanced example","title":"Topic Modeling with BERTopic","text":"See vignettes (including train_and_save_model.Rmd load_and_reuse_model.Rmd) Quarto tutorial complete workflow (training, representation models [keyBERT, ollama models, …], dimensionality reduction, clustering, visualizations).","code":""},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"scripts","dir":"","previous_headings":"","what":"Scripts","title":"Topic Modeling with BERTopic","text":"demo script available inst/scripts/train_model_function_demo.R shows end--end training, saving, loading, reuse.","code":""},{"path":[]},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Topic Modeling with BERTopic","text":"BERTopic described :","code":"@article{grootendorst2022bertopic,   title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},   author={Grootendorst, Maarten},   journal={arXiv preprint arXiv:2203.05794},   year={2022} }"},{"path":"https://tpetric7.github.io/bertopicr/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Topic Modeling with BERTopic","text":"package licensed MIT License. free use, modify, distribute software, provided proper attribution given original author.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/configure_macos_homebrew_zlib.html","id":null,"dir":"Reference","previous_headings":"","what":"Configure Homebrew zlib on macOS — configure_macos_homebrew_zlib","title":"Configure Homebrew zlib on macOS — configure_macos_homebrew_zlib","text":"Sets DYLD_FALLBACK_LIBRARY_PATH Homebrew's zlib lib directory. can help reticulate find compatible libraries macOS.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/configure_macos_homebrew_zlib.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configure Homebrew zlib on macOS — configure_macos_homebrew_zlib","text":"","code":"configure_macos_homebrew_zlib(quiet = FALSE)"},{"path":"https://tpetric7.github.io/bertopicr/reference/configure_macos_homebrew_zlib.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configure Homebrew zlib on macOS — configure_macos_homebrew_zlib","text":"quiet Logical. TRUE, suppresses messages.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/configure_macos_homebrew_zlib.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configure Homebrew zlib on macOS — configure_macos_homebrew_zlib","text":"Logical. TRUE environment updated, FALSE otherwise.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/find_topics_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Topics DataFrame Function — find_topics_df","title":"Find Topics DataFrame Function — find_topics_df","text":"function finds similar topics given keywords using BERTopic model returns results data frame tibble format.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/find_topics_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Topics DataFrame Function — find_topics_df","text":"","code":"find_topics_df(model, queries, top_n = 10, return_tibble = TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/reference/find_topics_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Topics DataFrame Function — find_topics_df","text":"model BERTopic model object. Must passed calling environment. queries vector keywords phrases query topics . top_n Number top similar topics retrieve query. Default 10. return_tibble Logical. TRUE, returns tibble. FALSE, returns data.frame. Default TRUE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/find_topics_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Topics DataFrame Function — find_topics_df","text":"data.frame tibble columns keyword, topics, similarity scores query.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/find_topics_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Topics DataFrame Function — find_topics_df","text":"","code":"# Example of finding similar topics using a BERTopic model if (exists(\"topic_model\")) {   queries <- c(\"national minority\", \"minority issues\", \"nationality issues\")   find_topics_df(model = topic_model, queries = queries, top_n = 10) } else {   message(\"No topic_model found. Please load a BERTopic model and try again.\") } #> No topic_model found. Please load a BERTopic model and try again."},{"path":"https://tpetric7.github.io/bertopicr/reference/get_document_info_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Document Information DataFrame — get_document_info_df","title":"Get Document Information DataFrame — get_document_info_df","text":"function retrieves document information BERTopic model processes unnest list columns, replace NA values, consolidate columns prefix.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_document_info_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Document Information DataFrame — get_document_info_df","text":"","code":"get_document_info_df(model, texts, drop_expanded_columns = TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_document_info_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Document Information DataFrame — get_document_info_df","text":"model BERTopic model object. texts character vector containing preprocessed texts passed BERTopic model. drop_expanded_columns Logical. TRUE, drops expanded columns consolidation. Default TRUE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_document_info_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Document Information DataFrame — get_document_info_df","text":"data.frame tibble unnested consolidated columns.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_document_info_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Document Information DataFrame — get_document_info_df","text":"","code":"if (FALSE) { # \\dontrun{ document_info_df <- get_document_info_df(model = topic_model, texts = texts_cleaned, drop_expanded_columns = TRUE) print(document_info_df) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_most_representative_docs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Most Representative Documents for a Specific Topic — get_most_representative_docs","title":"Get Most Representative Documents for a Specific Topic — get_most_representative_docs","text":"function filters given data frame select representative documents specified topic based probability scores. documents sorted relevance descending order, top n documents returned.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_most_representative_docs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Most Representative Documents for a Specific Topic — get_most_representative_docs","text":"","code":"get_most_representative_docs(df, topic_nr, n_docs = 5)"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_most_representative_docs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Most Representative Documents for a Specific Topic — get_most_representative_docs","text":"df data frame containing least columns 'Topic', 'Document', 'probs'. topic_nr integer specifying topic number filter documents. n_docs integer specifying number top representative documents return. Defaults 5.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_most_representative_docs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Most Representative Documents for a Specific Topic — get_most_representative_docs","text":"vector representative documents corresponding specified topic. number documents available less n_docs, available documents returned.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_most_representative_docs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Most Representative Documents for a Specific Topic — get_most_representative_docs","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming `df_docs` is a data frame with columns `Topic`, `Document`, and `probs` get_most_representative_docs(df_docs, topic_nr = 3, n_docs = 5) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_representative_docs_custom.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Representative Documents for a Specific Topic — get_representative_docs_custom","title":"Get Representative Documents for a Specific Topic — get_representative_docs_custom","text":"function filters given data frame select specified number representative documents particular topic. uses random sampling select documents.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_representative_docs_custom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Representative Documents for a Specific Topic — get_representative_docs_custom","text":"","code":"get_representative_docs_custom(df, topic_nr, n_docs)"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_representative_docs_custom.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Representative Documents for a Specific Topic — get_representative_docs_custom","text":"df data frame containing least columns 'Topic' 'Document'. topic_nr integer specifying topic number filter documents. n_docs integer specifying number documents sample specified topic.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_representative_docs_custom.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Representative Documents for a Specific Topic — get_representative_docs_custom","text":"vector sampled documents corresponding specified topic.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_representative_docs_custom.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Representative Documents for a Specific Topic — get_representative_docs_custom","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming `df_docs` is a data frame with columns `Topic`, `Document`, and `probs` get_representative_docs_custom(df_docs, topic_nr = 3, n_docs = 5) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Topic DataFrame Function — get_topic_df","title":"Get Topic DataFrame Function — get_topic_df","text":"function retrieves specified number words high probability given topic number BERTopic model returns results data frame tibble format.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Topic DataFrame Function — get_topic_df","text":"","code":"get_topic_df(model, topic_number = 0, top_n = 10, return_tibble = TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Topic DataFrame Function — get_topic_df","text":"model BERTopic model object. Must passed calling environment. topic_number topic number words scores retrieved. top_n Number top words retrieve specified topic. Default 10. greater 10, set 10 BERTopic returns maximum 10 words. return_tibble Logical. TRUE, returns tibble. FALSE, returns data.frame. Default TRUE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Topic DataFrame Function — get_topic_df","text":"data.frame tibble columns word, score, topic number.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Topic DataFrame Function — get_topic_df","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage: if (exists(\"topic_model\")) {   topic_df <- get_topic_df(model = topic_model, topic_number = 3, top_n = 5)   print(topic_df) } else {   message(\"No topic_model found. Please load a BERTopic model and try again.\") } } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_info_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Topic Information DataFrame — get_topic_info_df","title":"Get Topic Information DataFrame — get_topic_info_df","text":"function retrieves topic information BERTopic model processes unnest list columns, replace NA values, consolidate columns prefix.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_info_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Topic Information DataFrame — get_topic_info_df","text":"","code":"get_topic_info_df(model, drop_expanded_columns = TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_info_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Topic Information DataFrame — get_topic_info_df","text":"model BERTopic model object. drop_expanded_columns Logical. TRUE, drops expanded columns consolidation. Default TRUE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_info_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Topic Information DataFrame — get_topic_info_df","text":"data.frame tibble unnested consolidated columns.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topic_info_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Topic Information DataFrame — get_topic_info_df","text":"","code":"if (FALSE) { # \\dontrun{ topic_info_df <- get_topic_info_df(model = topic_model, drop_expanded_columns = TRUE) print(topic_info_df) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topics_df.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Topics DataFrame Function — get_topics_df","title":"Get Topics DataFrame Function — get_topics_df","text":"function retrieves topics BERTopic model converts data frame tibble format.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topics_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Topics DataFrame Function — get_topics_df","text":"","code":"get_topics_df(model, return_tibble = TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topics_df.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Topics DataFrame Function — get_topics_df","text":"model BERTopic model object. Must passed calling environment. return_tibble Logical. TRUE, returns tibble. FALSE, returns data.frame. Default TRUE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topics_df.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Topics DataFrame Function — get_topics_df","text":"data.frame tibble columns word, score, topic number across topics.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/get_topics_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Topics DataFrame Function — get_topics_df","text":"","code":"if (FALSE) { # \\dontrun{ topics_df <- get_topics_df(model = topic_model) print(topics_df) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/load_bertopic_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a BERTopic Model Bundle — load_bertopic_model","title":"Load a BERTopic Model Bundle — load_bertopic_model","text":"Load BERTopic model saved save_bertopic_model() along companion RDS file containing R-side extras.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/load_bertopic_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a BERTopic Model Bundle — load_bertopic_model","text":"","code":"load_bertopic_model(path, embedding_model = NULL)"},{"path":"https://tpetric7.github.io/bertopicr/reference/load_bertopic_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a BERTopic Model Bundle — load_bertopic_model","text":"path Directory path Python model saved. embedding_model Optional embedding model pass BERTopic$load() embedding model serialized.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/load_bertopic_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a BERTopic Model Bundle — load_bertopic_model","text":"list two elements: model (BERTopic model) extras (R-side data saved companion RDS file).","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/load_bertopic_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load a BERTopic Model Bundle — load_bertopic_model","text":"","code":"if (FALSE) { # \\dontrun{ loaded <- load_bertopic_model(\"topic_model\") doc_info <- get_document_info_df(model = loaded$model, texts = docs) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/save_bertopic_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Save a BERTopic Model Bundle — save_bertopic_model","title":"Save a BERTopic Model Bundle — save_bertopic_model","text":"Persist trained BERTopic model disk store R-side extras companion RDS file. recommended way reuse model across sessions working reticulate.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/save_bertopic_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save a BERTopic Model Bundle — save_bertopic_model","text":"","code":"save_bertopic_model(topic_model, path)"},{"path":"https://tpetric7.github.io/bertopicr/reference/save_bertopic_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save a BERTopic Model Bundle — save_bertopic_model","text":"topic_model list returned train_bertopic_model(). Must contain Python BERTopic model topic_model$model. Optional extras probabilities, reduced embeddings, topics time, topics per class saved present set NULL otherwise. path Directory path write Python model . RDS companion file saved paste0(path, \"_extras.rds\").","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/save_bertopic_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save a BERTopic Model Bundle — save_bertopic_model","text":"Invisibly returns TRUE successful write.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/save_bertopic_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save a BERTopic Model Bundle — save_bertopic_model","text":"","code":"if (FALSE) { # \\dontrun{ save_bertopic_model(topic_model, \"topic_model\") } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/setup_python_environment.html","id":null,"dir":"Reference","previous_headings":"","what":"Set Up Python Environment for BERTopic — setup_python_environment","title":"Set Up Python Environment for BERTopic — setup_python_environment","text":"function sets Python environment required packages using BERTopic model within R package. can create activate virtualenv conda environment install bundled requirements.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/setup_python_environment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set Up Python Environment for BERTopic — setup_python_environment","text":"","code":"setup_python_environment(   envname = \"r-bertopic\",   python_path = NULL,   method = c(\"virtualenv\", \"conda\"),   python_version = NULL,   upgrade = TRUE,   extra_packages = NULL )"},{"path":"https://tpetric7.github.io/bertopicr/reference/setup_python_environment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set Up Python Environment for BERTopic — setup_python_environment","text":"envname name Python environment. Default \"r-bertopic\". python_path Optional path specific Python executable (virtualenv ). method Environment type create use. One \"virtualenv\" \"conda\". python_version Optional Python version conda (e.g. \"3.10\"). upgrade Logical. TRUE, passes –upgrade pip installs. Default TRUE. extra_packages Optional character vector additional Python packages install.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/setup_python_environment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set Up Python Environment for BERTopic — setup_python_environment","text":"Invisibly returns active Python configuration.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/train_bertopic_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Train a BERTopic Model — train_bertopic_model","title":"Train a BERTopic Model — train_bertopic_model","text":"function creates embeddings sentence-transformers, configures UMAP, HDBSCAN, CountVectorizer, optionally wires representation model, fits BERTopic model R. returned model can used bertopicr helpers.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/train_bertopic_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train a BERTopic Model — train_bertopic_model","text":"","code":"train_bertopic_model(   docs,   embedding_model = \"Qwen/Qwen3-Embedding-0.6B\",   embeddings = NULL,   embedding_batch_size = 32,   embedding_show_progress = TRUE,   umap_model = NULL,   umap_n_neighbors = 15,   umap_n_components = 5,   umap_min_dist = 0,   umap_metric = \"cosine\",   umap_random_state = 42,   hdbscan_model = NULL,   hdbscan_min_cluster_size = 50,   hdbscan_min_samples = 20,   hdbscan_metric = \"euclidean\",   hdbscan_cluster_selection_method = \"eom\",   hdbscan_gen_min_span_tree = TRUE,   hdbscan_prediction_data = TRUE,   hdbscan_core_dist_n_jobs = 1,   vectorizer_model = NULL,   stop_words = \"all_stopwords\",   ngram_range = c(1, 3),   min_df = 2L,   max_df = 50L,   max_features = 10000,   strip_accents = NULL,   decode_error = \"strict\",   encoding = \"UTF-8\",   representation_model = c(\"none\", \"keybert\", \"mmr\", \"ollama\"),   representation_params = list(),   ollama_model = NULL,   ollama_base_url = \"http://localhost:11434/v1\",   ollama_api_key = \"ollama\",   ollama_client_params = list(),   ollama_prompt = NULL,   top_n_words = 200L,   calculate_probabilities = TRUE,   verbose = TRUE,   seed = NULL,   timestamps = NULL,   topics_over_time_nr_bins = 20L,   topics_over_time_global_tuning = TRUE,   topics_over_time_evolution_tuning = TRUE,   classes = NULL,   compute_reduced_embeddings = TRUE,   reduced_embedding_n_neighbors = 10L,   reduced_embedding_min_dist = 0,   reduced_embedding_metric = \"cosine\",   compute_hierarchical_topics = TRUE,   bertopic_args = list() )"},{"path":"https://tpetric7.github.io/bertopicr/reference/train_bertopic_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train a BERTopic Model — train_bertopic_model","text":"docs Character vector documents model. embedding_model Sentence-transformers model name local path. embeddings Optional precomputed embeddings (matrix array). embedding_batch_size Batch size embedding encoding. embedding_show_progress Logical. Show embedding progress bar. umap_model Optional pre-built UMAP Python object. NULL, one created. umap_n_neighbors Number neighbors UMAP. umap_n_components Number UMAP components. umap_min_dist UMAP min_dist parameter. umap_metric UMAP metric. umap_random_state Random state UMAP. hdbscan_model Optional pre-built HDBSCAN Python object. NULL, one created. hdbscan_min_cluster_size HDBSCAN min_cluster_size. hdbscan_min_samples HDBSCAN min_samples. hdbscan_metric HDBSCAN metric. hdbscan_cluster_selection_method HDBSCAN cluster selection method. hdbscan_gen_min_span_tree HDBSCAN gen_min_span_tree. hdbscan_prediction_data Logical. Whether generate prediction data. hdbscan_core_dist_n_jobs HDBSCAN core_dist_n_jobs. vectorizer_model Optional pre-built CountVectorizer Python object. stop_words Stop words CountVectorizer. Use \"all_stopwords\" load bundled multilingual list, \"english\", character vector. ngram_range Length-2 integer vector n-gram range. min_df Minimum document frequency CountVectorizer. max_df Maximum document frequency CountVectorizer. max_features Maximum features CountVectorizer. strip_accents Passed CountVectorizer. Use NULL preserve umlauts. decode_error Passed CountVectorizer decoding input bytes. encoding Text encoding CountVectorizer (defaults \"utf-8\"). representation_model Representation model use: \"none\", \"keybert\", \"mmr\", \"ollama\". representation_params Named list parameters passed representation model. ollama_model Ollama model name representation_model = \"ollama\". ollama_base_url Base URL Ollama OpenAI-compatible endpoint. ollama_api_key API key placeholder Ollama OpenAI-compatible endpoint. ollama_client_params Named list extra parameters passed openai$OpenAI(). ollama_prompt Optional prompt template Ollama OpenAI representation. top_n_words Number top words per topic keep model. calculate_probabilities Logical. Whether calculate topic probabilities. verbose Logical. Verbosity BERTopic. seed Optional random seed. timestamps Optional vector timestamps (Date/POSIXt/ISO strings integer) topics time. Defaults NULL (topics time disabled). topics_over_time_nr_bins Number bins topics_over_time. topics_over_time_global_tuning Logical. Whether enable global tuning topics_over_time. topics_over_time_evolution_tuning Logical. Whether enable evolution tuning topics_over_time. classes Optional vector class labels (character factor) topics per class. Defaults NULL (topics per class disabled). compute_reduced_embeddings Logical. TRUE, computes 2D 3D UMAP reductions. reduced_embedding_n_neighbors Number neighbors reduced embeddings. reduced_embedding_min_dist UMAP min_dist reduced embeddings. reduced_embedding_metric UMAP metric reduced embeddings. compute_hierarchical_topics Logical. TRUE, computes hierarchical topics. bertopic_args Named list extra arguments passed BERTopic().","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/train_bertopic_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train a BERTopic Model — train_bertopic_model","text":"list elements model, topics, probabilities, embeddings, reduced_embeddings_2d, reduced_embeddings_3d, hierarchical_topics, topics_over_time, topics_per_class.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/train_bertopic_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train a BERTopic Model — train_bertopic_model","text":"","code":"if (FALSE) { # \\dontrun{ setup_python_environment() texts <- c(\"Cats are great pets\", \"Dogs are loyal companions\", \"Markets fluctuate\") fit <- train_bertopic_model(texts, embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\") visualize_topics(fit$model, filename = \"intertopic_distance_map\", auto_open = FALSE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_barchart.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize BERTopic Bar Chart — visualize_barchart","title":"Visualize BERTopic Bar Chart — visualize_barchart","text":"function visualizes topics BERTopic model using Plotly saves output interactive HTML file. checks required Python modules allows custom file naming.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_barchart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize BERTopic Bar Chart — visualize_barchart","text":"","code":"visualize_barchart(   model,   filename = \"topics_topwords_interactive_barchart\",   open_file = FALSE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_barchart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize BERTopic Bar Chart — visualize_barchart","text":"model BERTopic model object. Must passed calling environment. filename character string specifying name HTML file save bar chart. Default \"topics_topwords_interactive_barchart\". .html extension added automatically provided. open_file Logical. TRUE, opens HTML file saving. Default FALSE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_barchart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize BERTopic Bar Chart — visualize_barchart","text":"Displays interactive bar chart within R environment saves HTML file.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_barchart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize BERTopic Bar Chart — visualize_barchart","text":"","code":"if (FALSE) { # \\dontrun{ visualize_barchart(model = topic_model, filename = \"custom_barchart\", open_file = TRUE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Topic Distribution for a Specific Document using BERTopic — visualize_distribution","title":"Visualize Topic Distribution for a Specific Document using BERTopic — visualize_distribution","text":"function visualizes topic distribution specific document BERTopic model using Python's Plotly library. visualization saved interactive HTML file, can opened viewed web browser.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Topic Distribution for a Specific Document using BERTopic — visualize_distribution","text":"","code":"visualize_distribution(   model,   text_id = 1,   probabilities,   filename = \"topic_dist_interactive\",   auto_open = FALSE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Topic Distribution for a Specific Document using BERTopic — visualize_distribution","text":"model BERTopic model object. model must method visualize_distribution. text_id integer specifying index document topic distribution visualized. Default 1. Must positive integer valid index within probabilities matrix. probabilities matrix data frame topic probabilities, rows corresponding documents columns topics. element represents probability topic given document. filename character string specifying name HTML file save visualization. Default \"topic_dist_interactive\". .html extension added automatically. auto_open Logical. TRUE, HTML file automatically open browser. Default FALSE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Topic Distribution for a Specific Document using BERTopic — visualize_distribution","text":"function return value saves HTML file containing visualization displays current R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Topic Distribution for a Specific Document using BERTopic — visualize_distribution","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'topic_model' is a BERTopic model object and 'probs' is a matrix of topic probabilities visualize_distribution(   model = topic_model,   text_id = 1,   probabilities = probs,   filename = \"custom_filename\",   auto_open = TRUE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Documents in Reduced Embedding Space — visualize_documents","title":"Visualize Documents in Reduced Embedding Space — visualize_documents","text":"function generates visualization documents using pre-trained BERTopic model. uses UMAP reduce dimensionality embeddings Plotly interactive visualizations.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Documents in Reduced Embedding Space — visualize_documents","text":"","code":"visualize_documents(   model = topic_model,   texts = texts_cleaned,   reduced_embeddings = reduced_embeddings,   custom_labels = FALSE,   hide_annotation = TRUE,   filename = \"visualize_documents\",   auto_open = FALSE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Documents in Reduced Embedding Space — visualize_documents","text":"model BERTopic model object. Default 'topic_model'. texts list vector cleaned text documents visualize. Default 'texts_cleaned'. reduced_embeddings matrix reduced-dimensionality embeddings. Typically generated using UMAP. Default 'reduced_embeddings'. custom_labels logical value indicating whether use custom labels topics. Default FALSE. hide_annotation logical value indicating whether hide annotations plot. Default TRUE. filename string specifying name HTML file save visualization. Default \"visualize_documents\". auto_open logical value indicating whether automatically open HTML file saving. Default FALSE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Documents in Reduced Embedding Space — visualize_documents","text":"Plotly visualization documents, displayed HTML file within R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Documents in Reduced Embedding Space — visualize_documents","text":"","code":"if (FALSE) { # \\dontrun{ visualize_documents(model = topic_model,                     texts = texts_cleaned,                     reduced_embeddings = reduced_embeddings,                     custom_labels = FALSE,                     hide_annotation = TRUE,                     filename = \"visualize_documents\",                     auto_open = FALSE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_2d.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Documents in 2D Space using BERTopic — visualize_documents_2d","title":"Visualize Documents in 2D Space using BERTopic — visualize_documents_2d","text":"function generates 3D visualization documents using pre-trained BERTopic model UMAP dimensionality reduction. uses Plotly interactive visualizations saves output HTML file.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_2d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Documents in 2D Space using BERTopic — visualize_documents_2d","text":"","code":"visualize_documents_2d(   model,   texts,   reduced_embeddings,   custom_labels = FALSE,   hide_annotation = TRUE,   tooltips = c(\"Topic\", \"Name\", \"Probability\", \"Text\"),   filename = \"visualize_documents_2d\",   auto_open = FALSE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_2d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Documents in 2D Space using BERTopic — visualize_documents_2d","text":"model BERTopic model object. Default 'topic_model'. texts character vector list cleaned text documents visualize. reduced_embeddings matrix data frame reduced-dimensionality embeddings (2D). Typically generated using UMAP. custom_labels Logical. TRUE, custom topic labels used. Default FALSE. hide_annotation Logical. TRUE, hides annotations plot. Default TRUE. tooltips character vector tooltips hover information. Default c(\"Topic\", \"Name\", \"Probability\", \"Text\"). filename character string specifying name HTML file save visualization. Default \"visualize_documents_2d\". .html extension automatically added provided. auto_open Logical. TRUE, opens HTML file browser saving. Default FALSE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_2d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Documents in 2D Space using BERTopic — visualize_documents_2d","text":"function return value saves HTML file containing visualization displays current R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_2d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Documents in 2D Space using BERTopic — visualize_documents_2d","text":"","code":"if (FALSE) { # \\dontrun{ visualize_documents_2d(model = topic_model,   texts = texts_cleaned,   reduced_embeddings = embeddings,   custom_labels = FALSE,   hide_annotation = TRUE,   filename = \"plot\",   auto_open = TRUE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_3d.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Documents in 3D Space using BERTopic — visualize_documents_3d","title":"Visualize Documents in 3D Space using BERTopic — visualize_documents_3d","text":"function generates 3D visualization documents using pre-trained BERTopic model UMAP dimensionality reduction. uses Plotly interactive visualizations saves output HTML file.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_3d.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Documents in 3D Space using BERTopic — visualize_documents_3d","text":"","code":"visualize_documents_3d(   model,   texts,   reduced_embeddings,   custom_labels = FALSE,   hide_annotation = TRUE,   tooltips = c(\"Topic\", \"Name\", \"Probability\", \"Text\"),   filename = \"visualize_documents_3d\",   auto_open = FALSE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_3d.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Documents in 3D Space using BERTopic — visualize_documents_3d","text":"model BERTopic model object. Default 'topic_model'. texts character vector list cleaned text documents visualize. reduced_embeddings matrix data frame reduced-dimensionality embeddings (3D). Typically generated using UMAP. custom_labels Logical. TRUE, custom topic labels used. Default FALSE. hide_annotation Logical. TRUE, hides annotations plot. Default TRUE. tooltips character vector tooltips hover information. Default c(\"Topic\", \"Name\", \"Probability\", \"Text\"). filename character string specifying name HTML file save visualization. Default \"visualize_documents_3d\". .html extension automatically added provided. auto_open Logical. TRUE, opens HTML file browser saving. Default FALSE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_3d.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Documents in 3D Space using BERTopic — visualize_documents_3d","text":"function return value saves HTML file containing visualization displays current R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_documents_3d.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Documents in 3D Space using BERTopic — visualize_documents_3d","text":"","code":"if (FALSE) { # \\dontrun{ visualize_documents_3d(model = topic_model,   texts = texts_cleaned,   reduced_embeddings = embeddings,   custom_labels = FALSE,   hide_annotation = TRUE,   filename = \"plot\",   auto_open = TRUE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_heatmap.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Topic Similarity Heatmap using BERTopic — visualize_heatmap","title":"Visualize Topic Similarity Heatmap using BERTopic — visualize_heatmap","text":"function visualizes topic similarity heatmap topics BERTopic model using Python's Plotly library. visualization saved interactive HTML file, can opened viewed web browser.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_heatmap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Topic Similarity Heatmap using BERTopic — visualize_heatmap","text":"","code":"visualize_heatmap(   model,   filename = \"topics_similarity_heatmap\",   auto_open = FALSE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_heatmap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Topic Similarity Heatmap using BERTopic — visualize_heatmap","text":"model BERTopic model object. model must method visualize_heatmap. filename character string specifying name HTML file save visualization. default value \"topics_similarity_heatmap\". filename contain illegal characters. .html extension added automatically provided. auto_open Logical. TRUE, opens HTML file saving. Default FALSE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_heatmap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Topic Similarity Heatmap using BERTopic — visualize_heatmap","text":"function return value saves HTML file containing visualization displays current R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_heatmap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Topic Similarity Heatmap using BERTopic — visualize_heatmap","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'topic_model' is a BERTopic model object visualize_heatmap(model = topic_model, filename = \"topics_similarity_heatmap\", auto_open = FALSE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_hierarchy.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Topic Hierarchy Nodes using BERTopic — visualize_hierarchy","title":"Visualize Topic Hierarchy Nodes using BERTopic — visualize_hierarchy","text":"function visualizes hierarchical clustering topics BERTopic model. hierarchical topics DataFrame provided, uses visualization; otherwise, visualizes directly model. visualization saved interactive HTML file, can opened viewed web browser.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_hierarchy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Topic Hierarchy Nodes using BERTopic — visualize_hierarchy","text":"","code":"visualize_hierarchy(   model,   hierarchical_topics = NULL,   filename = \"topic_hierarchy\",   auto_open = TRUE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_hierarchy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Topic Hierarchy Nodes using BERTopic — visualize_hierarchy","text":"model BERTopic model object. model must method visualize_hierarchy. hierarchical_topics Optional. hierarchical topics DataFrame created using BERTopic model's hierarchical_topics method. provided, object used generate hierarchy visualization. filename character string specifying name HTML file save visualization. default value \"topic_hierarchy\". filename contain illegal characters. auto_open Logical. TRUE, HTML file opened automatically saved. Default TRUE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_hierarchy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Topic Hierarchy Nodes using BERTopic — visualize_hierarchy","text":"function return value saves HTML file containing visualization displays current R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_hierarchy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Topic Hierarchy Nodes using BERTopic — visualize_hierarchy","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'topic_model' is a BERTopic model object visualize_hierarchy(model = topic_model, filename = \"topic_hierarchy\", auto_open = TRUE)  # Alternatively, provide a pre-calculated hierarchical_topics object visualize_hierarchy(model = topic_model, hierarchical_topics = hierarchical_topics, filename = \"topic_hierarchy\", auto_open = TRUE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Topics using BERTopic — visualize_topics","title":"Visualize Topics using BERTopic — visualize_topics","text":"function visualizes intertopic distance map topics BERTopic model using Python's Plotly library. visualization saved interactive HTML file, can opened viewed web browser.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Topics using BERTopic — visualize_topics","text":"","code":"visualize_topics(   model,   filename = \"intertopic_distance_map\",   auto_open = FALSE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Topics using BERTopic — visualize_topics","text":"model BERTopic model object. model must method visualize_topics. filename character string specifying name HTML file save visualization. default value \"intertopic_distance_map\". filename contain illegal characters. .html extension added automatically provided. auto_open Logical. TRUE, opens HTML file saving. Default FALSE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Topics using BERTopic — visualize_topics","text":"function return value saves HTML file containing visualization displays current R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Topics using BERTopic — visualize_topics","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'topic_model' is a BERTopic model object visualize_topics(model = topic_model, filename = \"plot\", auto_open = TRUE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_over_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Topics Over Time using BERTopic — visualize_topics_over_time","title":"Visualize Topics Over Time using BERTopic — visualize_topics_over_time","text":"function visualizes topics time BERTopic model using Python's Plotly library. visualization saved interactive HTML file, can opened viewed web browser.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_over_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Topics Over Time using BERTopic — visualize_topics_over_time","text":"","code":"visualize_topics_over_time(   model,   topics_over_time_model,   top_n_topics = 20,   filename = \"topics_over_time\" )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_over_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Topics Over Time using BERTopic — visualize_topics_over_time","text":"model BERTopic model object. model must method visualize_topics_over_time. topics_over_time_model topics--time model object created using BERTopic model. top_n_topics integer specifying number top topics display visualization. Default 20. Must positive integer. filename character string specifying name HTML file save visualization. default value \"topics_over_time\". filename contain illegal characters.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_over_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Topics Over Time using BERTopic — visualize_topics_over_time","text":"function return value saves HTML file containing visualization displays current R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_over_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Topics Over Time using BERTopic — visualize_topics_over_time","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming 'topics_over_time_model' is a BERTopic model object visualize_topics_over_time(model = topic_model,                            topics_over_time_model = topics_over_time,                            top_n_topics = 5,                            filename = \"plot\") } # }"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_per_class.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Topics per Class — visualize_topics_per_class","title":"Visualize Topics per Class — visualize_topics_per_class","text":"function visualizes distribution topics per class using pre-trained BERTopic model. visualization generated using Plotly Python package displayed within R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_per_class.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Topics per Class — visualize_topics_per_class","text":"","code":"visualize_topics_per_class(   model = topic_model,   topics_per_class = topics_per_class,   start = 0,   end = 10,   filename = \"topics_per_class\",   auto_open = TRUE )"},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_per_class.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Topics per Class — visualize_topics_per_class","text":"model BERTopic model object. Default 'topic_model'. topics_per_class data frame list containing topics per class data. Default 'topics_per_class'. start integer specifying starting index topics visualize. Default 0. end integer specifying ending index topics visualize. Default 10. filename string specifying name HTML file save visualization. Default \"topics_per_class\". auto_open logical value indicating whether automatically open HTML file saving. Default TRUE.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_per_class.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Topics per Class — visualize_topics_per_class","text":"Plotly visualization topics per class, displayed HTML file within R environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/reference/visualize_topics_per_class.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Topics per Class — visualize_topics_per_class","text":"","code":"if (FALSE) { # \\dontrun{ visualize_topics_per_class(model = topic_model,                            topics_per_class = topics_per_class,                            start = 0, end = 7,                            filename = \"plot\",                            auto_open = TRUE) } # }"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Env & Packages","title":null,"text":"Load R packages initialize Python environment reticulate package. default, reticulate uses isolated Python virtual environment named r-reticulate (cf. https://rstudio.github.io/reticulate/). use_python() function enables specify alternate Python (cf. https://rstudio.github.io/reticulate/).","code":"library(dplyr) Attaching package: 'dplyr' The following objects are masked from 'package:stats':      filter, lag The following objects are masked from 'package:base':      intersect, setdiff, setequal, union library(tidyr) library(purrr) library(utils) library(tibble) library(readr) library(tictoc) library(htmltools) library(arrow) Attaching package: 'arrow' The following object is masked from 'package:utils':      timestamp # interface with Python library(reticulate) # ~ Home, in cmd terminal insert: echo %USERPROFILE% userprofile <- Sys.getenv(\"USERPROFILE\") use_python(file.path(userprofile, \"anaconda3/envs/bertopic\"), required = TRUE) # use_python(\"path/to/your/python/env/bertopic\", required = TRUE) reticulate::py_config() python:         C:/Users/teodo/anaconda3/envs/bertopic/python.exe libpython:      C:/Users/teodo/anaconda3/envs/bertopic/python311.dll pythonhome:     C:/Users/teodo/anaconda3/envs/bertopic version:        3.11.12 | packaged by conda-forge | (main, Apr 10 2025, 22:09:00) [MSC v.1943 64 bit (AMD64)] Architecture:   64bit numpy:          C:/Users/teodo/anaconda3/envs/bertopic/Lib/site-packages/numpy numpy_version:  1.26.4  NOTE: Python version was forced by use_python() function reticulate::py_available() [1] TRUE library(bertopicr)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Text preparation","title":null,"text":"need arrow package open parquet file . German texts text_clean column. segmented smaller chunks (100 tokens long) optimize topic extraction Bertopic. Special characters removed cleaning function. text chunks lower case. collected stopword list includes German English tokens inserted Python’s CountVectorizer c-TF-IDF calculation. language model fed text chunks text_clean column stopword removal. texts_cleaned timesteps lists, need model preparation, topic extraction visualization.","code":"# Find the path # parquet_path <- system.file(\"data\", \"spiegel_sample.parquet\", package = \"bertopicr\") # stopwords_path <- system.file(\"data\", \"all_stopwords.txt\", package = \"bertopicr\") parquet_path <- file.path(\"inst/extdata\", \"spiegel_sample.parquet\")  # Read the parquet file using the correct path library(arrow) dataset <- arrow::read_parquet(parquet_path)  names(dataset) [1] \"seq_id\"      \"doc_id\"      \"sentence_id\" \"genre\"       \"text_type\"    [6] \"author\"      \"title\"       \"Text\"        \"text_clean\"  \"date\" dim(dataset) [1] 3000   10 stopwords_path <- file.path(\"inst/extdata\", \"all_stopwords.txt\") all_stopwords <- read_lines(stopwords_path) texts_cleaned = dataset$text_clean titles = dataset$doc_id timestamps <- as.list(dataset$date) # timestamps <- as.integer(dataset$year)  texts_cleaned[[1]] [1] \"leserinnen und leser der lokaljournalismus leert sich schleichend aber unerbittlich öffentliche hilfe sollte hier ansetzen noch wichtiger hier sollten die mittel auch ankommen und das stellt die demokratische medienrettung vor ein dilemma aus guten gründen wollen unabhängige verlage keine staatlichen subventionen für journalismus es reicht wenn die öffentlich rechtlichen sender mit dem schwierigen spannungsverhältnis von rundfunkbeiträgen aufsichtsgremien und parteiinteressen zu kämpfen haben zugleich ist die bezuschussung von zeitungszustellern wie sie die verlage vorschlagen und der bundestag mit einem kleinen anteil schon vor corona beschlossen hat eine merkwürdige strukturkonservierung pizzakuriere werden ja auch nicht bezuschusst obwohl sie lebensmittel unter die leute bringen\""},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Model Preparation","title":null,"text":"model preparation, going use reticulate interface Python modules. R code essentially conversion Python code. Topic model preparation also include local language models (via ollama lm-studio) leveraging OpenAI endpoint. running program chunk , make sure install necessary Python packages virtual environment conda environment (see short instructions README file tutorials online): bertopic, numpy, scikit-learn, sentence-transformers, umap-learn, hdbscan, spacy, openai datetime. Download install ollama lm-studio computer serve local language model. RStudio, can accomplish installation Python packages reticulate commands: e.g., reticulate::py_install (\"bertopic\", envname = \"CHOOSE PATH PYTHON ENVIRONMENT\"). familiar working (windows conda) terminal, can activate appropriate python conda environment install necessary packages: e.g., pip install bertopic). First, import necessary Python packages: numpy, umap, hdbscan, scikit-learn, sentence_transformers, bertopic. SentenceTransformer creates necessary embeddings (vector representations text tokens) topic modeling bertopic. first time SentenceTransformer used specific model, model downloaded huggingface website (https://huggingface.co/), many freely usable language models hosted (https://huggingface.co/models). next two steps, umap module reduces number dimensions embeddings, hdbscan module extracts clusters can evaluated topic pipeline. dimension reduction methods (like PCA tSNE) can used instead. hdbscan module extracts clusters can evaluated topic pipeline. clustering methods (like kmeans) can used instead. Countvectorizer calculates c-TF-IDF frequencies enables representation model defined extract suitable keywords descriptors extracted topics. Stopwords removed creating embeddings, keyword extraction. Stopword removal accomplished CountVectorizer option. example , multiple representation models used keyword extraction identified topics topic description: keyBERT (part bertopic), language model (served locally ollama online Groq, via OpenAI endpoint), Maximal Marginal Relevance model (MMR) spacy POS representation model. default, one Representation model created bertopic. prompt describes task language model accomplish, mentions documents work topic labels derive text contents keywords. Bertopic enables us define zeroshot list keywords can used drive topic model towards desired topic outcomes. code , zeroshot keyword search disabled, can activated needed. preparational steps, topic model trained : topic_model$fit_transform(texts, embeddings). obtain topic labels topics <- fit_transform[[1]] topic probabilities probs <- fit_transform[[2]]. Since dataset contains time-related metadata, can use timestamps dynamic topic modeling, .e., discovering topic development topic sequences time. data doesn’t contain time-related column, timestamps topics_over_time calculations disabled. topic labels probabilities stored dataframe named results, together variables metadata.","code":"# Import necessary Python modules np <- import(\"numpy\") umap <- import(\"umap\") UMAP <- umap$UMAP hdbscan <- import(\"hdbscan\") HDBSCAN <- hdbscan$HDBSCAN sklearn <- import(\"sklearn\") CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer bertopic <- import(\"bertopic\") plotly <- import(\"plotly\") datetime <- import(\"datetime\") # Embed the sentences py <- import_builtins() sentence_transformers <- import(\"sentence_transformers\") SentenceTransformer <- sentence_transformers$SentenceTransformer embedding_model = SentenceTransformer(\"BAAI/bge-m3\") # for multiple languages embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE) # Initialize UMAP and HDBSCAN models umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L) hdbscan_model <- HDBSCAN(min_cluster_size=15L, min_samples = 10L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE, core_dist_n_jobs = 1) # Initialize CountVectorizer vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),                                      max_features = 10000L, max_df = 50L,                                     stop_words = all_stopwords) sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned) sentence_vectors_dense <- np$array(sentence_vectors) sentence_vectors_dense <- py_to_r(sentence_vectors_dense) # Initialize representation models keybert_model <- bertopic$representation$KeyBERTInspired() openai <- import(\"openai\") OpenAI <- openai$OpenAI ollama <- import(\"ollama\")  # Point to the local server (ollama or lm-studio) client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama') # client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')  prompt <- \" I have a topic that contains the following documents: [DOCUMENTS] The topic is described by the following keywords: [KEYWORDS]  Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format: topic: <topic label> \"  # download an appropriate local LLM for your computer with ollama/lm-studio # e.g., insert in your terminal 'ollama pull llama3.1' openai_model <- bertopic$representation$OpenAI(client,                                                 # model = \"qwen3\",                                                # model = \"gemma3:4b\",                                                model = \"qwen2.5:14b-instruct\",                                                # model = \"llama3.1:8b-instruct-fp16\",                                                exponential_backoff = TRUE,                                                 chat = TRUE,                                                 prompt = prompt)  # downlaod a spacy language model from spacy.io suitable for your computer  # before usage insert in your terminal 'python -m spacy download de_core_news_md' pos_model <- bertopic$representation$PartOfSpeech(\"de_core_news_md\") # diversity set relatively high to reduce repetition of keyword word forms mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)  # Combine all representation models representation_model <- list(   \"KeyBERT\" = keybert_model,   \"OpenAI\" = openai_model,   \"MMR\" = mmr_model,   \"POS\" = pos_model ) # We can define a number of topics that we know are in the documents zeroshot_topic_list  <- list(\"german national identity\", \"minority issues in germany\") # Initialize BERTopic model with pipeline models and hyperparameters BERTopic <- bertopic$BERTopic topic_model <- BERTopic(   embedding_model = embedding_model,   umap_model = umap_model,   hdbscan_model = hdbscan_model,   vectorizer_model = vectorizer_model,   # zeroshot_topic_list = zeroshot_topic_list,   # zeroshot_min_similarity = 0.85,   representation_model = representation_model,   calculate_probabilities = TRUE,   top_n_words = 10L,   verbose = TRUE ) tictoc::tic()  # Fit the model and transform the texts fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings) topics <- fit_transform[[1]] # probs <- fit_transform[[2]]  # Now transform the texts to get the updated probabilities transform_result <- topic_model$transform(texts_cleaned) probs <- transform_result[[2]]  # Extract the updated probabilities  tictoc::toc() 47.67 sec elapsed # Converting R Date to Python datetime datetime <- import(\"datetime\")  timestamps <- as.list(dataset$date) # timestamps <- as.integer(dataset$year)  # Convert each R date object to an ISO 8601 string timestamps <- lapply(timestamps, function(x) {   format(x, \"%Y-%m-%dT%H:%M:%S\")  # ISO 8601 format })  # Dynamic topic model topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE) # Combine results with additional columns results <- dataset |>    mutate(Topic = topics,           Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence  results <- results |>    mutate(row_id = row_number()) |>    select(row_id, everything())  head(results,10) |> rmarkdown::paged_table() results |>   saveRDS(\"data/spiegel_topic_results_df.rds\") results |>   write_parquet(\"data/spiegel_topic_results_df.parquet\") results |>   write_csv(\"data/spiegel_topic_results_df.csv\") results |>   writexl::write_xlsx(\"data/spiegel_topic_results_df.xlsx\")"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Python packages","title":null,"text":"First, import necessary Python packages: numpy, umap, hdbscan, scikit-learn, sentence_transformers, bertopic.","code":"# Import necessary Python modules np <- import(\"numpy\") umap <- import(\"umap\") UMAP <- umap$UMAP hdbscan <- import(\"hdbscan\") HDBSCAN <- hdbscan$HDBSCAN sklearn <- import(\"sklearn\") CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer bertopic <- import(\"bertopic\") plotly <- import(\"plotly\") datetime <- import(\"datetime\")"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Embeddings","title":null,"text":"SentenceTransformer creates necessary embeddings (vector representations text tokens) topic modeling bertopic. first time SentenceTransformer used specific model, model downloaded huggingface website (https://huggingface.co/), many freely usable language models hosted (https://huggingface.co/models).","code":"# Embed the sentences py <- import_builtins() sentence_transformers <- import(\"sentence_transformers\") SentenceTransformer <- sentence_transformers$SentenceTransformer embedding_model = SentenceTransformer(\"BAAI/bge-m3\") # for multiple languages embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Dimension reduction","title":null,"text":"next two steps, umap module reduces number dimensions embeddings, hdbscan module extracts clusters can evaluated topic pipeline. dimension reduction methods (like PCA tSNE) can used instead.","code":"# Initialize UMAP and HDBSCAN models umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Clustering","title":null,"text":"hdbscan module extracts clusters can evaluated topic pipeline. clustering methods (like kmeans) can used instead.","code":"hdbscan_model <- HDBSCAN(min_cluster_size=15L, min_samples = 10L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE, core_dist_n_jobs = 1)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"c-TF-IDF","title":null,"text":"Countvectorizer calculates c-TF-IDF frequencies enables representation model defined extract suitable keywords descriptors extracted topics. Stopwords removed creating embeddings, keyword extraction. Stopword removal accomplished CountVectorizer option.","code":"# Initialize CountVectorizer vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),                                      max_features = 10000L, max_df = 50L,                                     stop_words = all_stopwords) sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned) sentence_vectors_dense <- np$array(sentence_vectors) sentence_vectors_dense <- py_to_r(sentence_vectors_dense)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Representation models","title":null,"text":"example , multiple representation models used keyword extraction identified topics topic description: keyBERT (part bertopic), language model (served locally ollama online Groq, via OpenAI endpoint), Maximal Marginal Relevance model (MMR) spacy POS representation model. default, one Representation model created bertopic. prompt describes task language model accomplish, mentions documents work topic labels derive text contents keywords.","code":"# Initialize representation models keybert_model <- bertopic$representation$KeyBERTInspired() openai <- import(\"openai\") OpenAI <- openai$OpenAI ollama <- import(\"ollama\")  # Point to the local server (ollama or lm-studio) client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama') # client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')  prompt <- \" I have a topic that contains the following documents: [DOCUMENTS] The topic is described by the following keywords: [KEYWORDS]  Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format: topic: <topic label> \"  # download an appropriate local LLM for your computer with ollama/lm-studio # e.g., insert in your terminal 'ollama pull llama3.1' openai_model <- bertopic$representation$OpenAI(client,                                                 # model = \"qwen3\",                                                # model = \"gemma3:4b\",                                                model = \"qwen2.5:14b-instruct\",                                                # model = \"llama3.1:8b-instruct-fp16\",                                                exponential_backoff = TRUE,                                                 chat = TRUE,                                                 prompt = prompt)  # downlaod a spacy language model from spacy.io suitable for your computer  # before usage insert in your terminal 'python -m spacy download de_core_news_md' pos_model <- bertopic$representation$PartOfSpeech(\"de_core_news_md\") # diversity set relatively high to reduce repetition of keyword word forms mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)  # Combine all representation models representation_model <- list(   \"KeyBERT\" = keybert_model,   \"OpenAI\" = openai_model,   \"MMR\" = mmr_model,   \"POS\" = pos_model )"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Zeroshot keywords","title":null,"text":"Bertopic enables us define zeroshot list keywords can used drive topic model towards desired topic outcomes. code , zeroshot keyword search disabled, can activated needed.","code":"# We can define a number of topics that we know are in the documents zeroshot_topic_list  <- list(\"german national identity\", \"minority issues in germany\")"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic Model","title":null,"text":"","code":"# Initialize BERTopic model with pipeline models and hyperparameters BERTopic <- bertopic$BERTopic topic_model <- BERTopic(   embedding_model = embedding_model,   umap_model = umap_model,   hdbscan_model = hdbscan_model,   vectorizer_model = vectorizer_model,   # zeroshot_topic_list = zeroshot_topic_list,   # zeroshot_min_similarity = 0.85,   representation_model = representation_model,   calculate_probabilities = TRUE,   top_n_words = 10L,   verbose = TRUE )"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Model Training","title":null,"text":"preparational steps, topic model trained : topic_model$fit_transform(texts, embeddings). obtain topic labels topics <- fit_transform[[1]] topic probabilities probs <- fit_transform[[2]].","code":"tictoc::tic()  # Fit the model and transform the texts fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings) topics <- fit_transform[[1]] # probs <- fit_transform[[2]]  # Now transform the texts to get the updated probabilities transform_result <- topic_model$transform(texts_cleaned) probs <- transform_result[[2]]  # Extract the updated probabilities  tictoc::toc() 47.67 sec elapsed"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic Dynamics","title":null,"text":"Since dataset contains time-related metadata, can use timestamps dynamic topic modeling, .e., discovering topic development topic sequences time. data doesn’t contain time-related column, timestamps topics_over_time calculations disabled.","code":"# Converting R Date to Python datetime datetime <- import(\"datetime\")  timestamps <- as.list(dataset$date) # timestamps <- as.integer(dataset$year)  # Convert each R date object to an ISO 8601 string timestamps <- lapply(timestamps, function(x) {   format(x, \"%Y-%m-%dT%H:%M:%S\")  # ISO 8601 format })  # Dynamic topic model topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Store Results","title":null,"text":"topic labels probabilities stored dataframe named results, together variables metadata.","code":"# Combine results with additional columns results <- dataset |>    mutate(Topic = topics,           Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence  results <- results |>    mutate(row_id = row_number()) |>    select(row_id, everything())  head(results,10) |> rmarkdown::paged_table() results |>   saveRDS(\"data/spiegel_topic_results_df.rds\") results |>   write_parquet(\"data/spiegel_topic_results_df.parquet\") results |>   write_csv(\"data/spiegel_topic_results_df.csv\") results |>   writexl::write_xlsx(\"data/spiegel_topic_results_df.xlsx\")"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Results","title":null,"text":"R package bertopicr used section display topic modeling results form lists, data frames visualizations. names functions nearly Python package BERTopic. get_document_info_df() creates dataframe contains documents associated topics, characteristics keywords, probability scores, representative documents topic representation model results (e.g., keywords extracted KeyBERT, MMR, spacy, LLM descriptions topics). First, create data frame similar df_docs , contains columns Topic, Document probs. use get_most_representative_docs() function extract representative documents chosen topic. function get_topic_info_df() creates another useful data frame, extracted topics shows number associated documents (text chunks), topic id (Name), characeristic keywords according chosen representation models three (concatenated) representative documents. get_topics_df() function concentrates words associated certain topic probability scores. outliers (Topic = -1) usually included analysis. BERTopic offers function reduce number outliers update topic model. visualize_barchart() creates interactive barchart top five words frequently occurring topics.  might prefer create easy customizable barchart ggplot2 package using dataframe extracted get_topics_df() function run plotly library R ggplotly() function interactive barchart. find_topics_df() function useful semantic search. can identify topics associated chosen query multiple queries. get_topic_df() function creates dataframe extracts top words chosen topic. visualize_distribution() function produces interactive barchart displays associated topics chosen document (text chunk). probability scores help identify likely topic(s) document (text chunk).  semantic relatedness topics can displayed visualize_topics() function.  can create similarity matrix applying cosine similarities generated topic embeddings. resulting matrix indicates similar topics . visualize similarity matrix can use visualize_heatmap() function.  best way display relatedness documents (text chunks) visualize_hierarchy() function, creates interactive dendrogram. additional option creation hierarchical topics list can included interactive dendrogram enables user identify joint expressions.  visualize_documents() function displays identified clusters associated certain topic two dimensions. Usually, best reduce dimensionality embeddings UMAP (another dimension reduction method) produce intelligible visual results. interactive plot allows user select one clusters double-click mouse.  create interactive 3D plot, visualize_documents_3d function can used.  can also inspect chosen number topics develop certain period time. visualize_topics_over_time() function assumes timestamps, topic model topics time model already defined (e.g., model preparation step topic model training). timestamps need integers certain date format (see model preparation step ).  dataset includes categorical variables (groups, classes, etc.), can use visualize_topics_per_class() function display interactive barchart groups classes associated chosen topic. double-click mouse, user can choose single topic inspect frequency groups.","code":"library(bertopicr) document_info_df <- get_document_info_df(model = topic_model,                                           texts = texts_cleaned,                                           drop_expanded_columns = TRUE) document_info_df |> head() |> rmarkdown::paged_table() # Create a data frame similar to df_docs df_docs <- tibble(Topic = results$Topic,                   Document = results$text_clean,                   probs = results$Probability) rep_docs <- get_most_representative_docs(df = df_docs,                                           topic_nr = 3,                                           n_docs = 5) unique(rep_docs) [1] \"fast alle ein mit sensoren vollgestopftes gerät mit sich herumtragen und ein immer größerer teil des lebens online stattfindet kann man sehr viel messen plötzlich existieren datenströme die auskunft geben über das fahrverhalten weil die bewegungsdaten des smartphones analysiert werden können datenströme von kameras die auf einer baustelle jeden einzelnen bauarbeiter überwachen und die jeweiligen leistungen sekundengenau messen die geschichte des kapitalismus ist eine geschichte der aggressiven effizienz und effizienz beginnt zwangsläufig mit der messung im digitalkapitalismus wird man darum kämpfen müssen welche der effizienzen gehoben werden sollen und welche nicht gehoben werden dürfen weil sie mit den\"                                                                          [2] \"die meisten menschen meinen dass digitalisierung ist wenn man per app seine seele dem teufel verkaufen oder auf ein fußballspiel in japan wetten kann so ähnlich geht es auch mit der künstlichen intelligenz sie ist ungefähr so segensreich wie die in den fünfzigerjahren prognostizierten atomgetriebenen staubsauger oder die küchen roboter aus mister spocks altenheim und selbstverständlich wird ki uns zunächst die supertolle lebenserleichterung ermöglichen dass der kühlschrank ding weiß wann schulferien sind und dass sie für ihre kinder eine zweiwöchige reise gebucht haben was dazu führt dass der kühlschrank eine woche vor und eine woche nach ostern etwas weniger fruchtjoghurt bestellt\"                                                                                                 [3] \"sollten sehr vorsichtig sein mit der künstlichen intelligenz wenn ich raten sollte was die größte bedrohung unserer existenz darstellt wäre es künstliche intelligenz bumm es war nicht das erste mal dass sich musk kritisch gegenüber dieser technologieform geäußert hat aber auch nicht das letzte mal denn mitte november schrieb musk in einem inzwischen gelöschten aber als echt bestätigten kommentar auf der debattenseite edge org die geschwindigkeit der entwicklung der künstlichen intelligenz ist unglaublich schnell innerhalb der nächsten fünf maximal zehn jahre könnte etwas ernsthaft gefährliches passieren musk befürchtet dass die menschheit die kontrolle verliert über die technologie\"                                                                                                 [4] \"kontrolliert wird ein digitaler algorithmen staat hätte ganz andere möglichkeiten er könnte die fahrgeschwindigkeit eines jeden autos in echtzeit kontrollieren und theoretisch bußgelder direkt von den konten der verkehrssünder abbuchen in der privatwirtschaft haben sich solche lückenlosen überwachungssysteme teils schon etabliert bei amazon feuert ein algorithmus manchmal mitarbeitende wenn diese aus sicht des unternehmens nicht gut genug performen die mittel der kontrolle und einflussnahme könnten durch algorithmen exponentiell wachsen manche die gesellschaft ordnenden prinzipien könnten sich dadurch umkehren im gesundheitssystem und im versicherungswesen etwa etabliert sich schon jetzt ein neues verständnis des vorsorgeprinzips derzeit zahlen wir oft monatlich geld in einen\" [5] \"sondern eine maschine erzeugt sie zeigen augenscheinlich in öl gemalte perserkatzen die entsetzt traurig oder wütend auf ihre smartphones starren offenbar weil sie mit ihren bitcoins so viel geld verloren haben es ist als mache eine künstliche intelligenz einen witz auf kosten der kryptowährungs fangemeinde die anweisung die das bild erzeugte hat sich ein silicon valley unternehmer ausgedacht er gehört zu den immer noch recht wenigen menschen die dalle solche anweisungen geben dürfen die stiftung openai schaltet nach und nach menschen frei die sich auf eine warteliste setzen lassen und angeben warum sie zugang zu ihrem werkzeug dalle haben möchten diese zugangsbeschränkungen sind\" topic_info_df <- get_topic_info_df(model = topic_model,                                     drop_expanded_columns = TRUE) head(topic_info_df) |> rmarkdown::paged_table() topics_df <- get_topics_df(model = topic_model) head(topics_df, 30) # A tibble: 30 × 3    Word          Score Topic    <chr>         <dbl> <dbl>  1 menschen    0.0149     -1  2 deutschland 0.0132     -1  3 deutschen   0.0101     -1  4 leben       0.00911    -1  5 welt        0.00864    -1  6 leute       0.00825    -1  7 gut         0.00799    -1  8 zeit        0.00799    -1  9 gerade      0.00780    -1 10 deutsche    0.00735    -1 # ℹ 20 more rows visualize_barchart(model = topic_model,                     filename = \"topics_topwords_interactive_barchart\", # default                    open_file = FALSE) # TRUE enables output in browser library(ggplot2)  barchart <- topics_df |>    group_by(Topic) |>    filter(Topic >= 0 & Topic <= 8) |>    slice_head(n=5) |>    mutate(Topic = paste(\"Topic\", as.character(Topic)),           Word = reorder(Word, Score)) |>    ggplot(aes(Score, Word, fill = Topic)) +   geom_col() +   facet_wrap(~ Topic, scales = \"free\") +   theme(legend.position = \"none\")  library(plotly) ggplotly(barchart) find_topics_df(model = topic_model,                 queries = \"migration\", # user input                top_n = 10, # default                return_tibble = TRUE) # default # A tibble: 10 × 3    keyword   topics similarity    <chr>      <int>      <dbl>  1 migration     17      0.563  2 migration     21      0.511  3 migration     -1      0.508  4 migration      0      0.484  5 migration      2      0.482  6 migration     11      0.481  7 migration     14      0.477  8 migration     12      0.477  9 migration     24      0.473 10 migration     13      0.472 find_topics_df(model = topic_model,                                 queries = c(\"migranten\", \"asylanten\"),                                top_n = 5) # A tibble: 10 × 3    keyword   topics similarity    <chr>      <int>      <dbl>  1 migranten     17      0.635  2 migranten     -1      0.551  3 migranten     12      0.549  4 migranten     21      0.543  5 migranten      2      0.528  6 asylanten     17      0.561  7 asylanten     24      0.526  8 asylanten     12      0.503  9 asylanten     21      0.500 10 asylanten     -1      0.498 get_topic_df(model = topic_model,                             topic_number = 0,                             top_n = 5, # default is 10                            return_tibble = TRUE) # default # A tibble: 5 × 3   Word       Score Topic   <chr>      <dbl> <int> 1 merkel    0.0467     0 2 spd       0.0409     0 3 kanzlerin 0.0285     0 4 partei    0.0272     0 5 cdu       0.0265     0 # default filename: topic_dist_interactive.html visualize_distribution(model = topic_model,                         text_id = 1, # user input                        probabilities = probs, # see model training                        filename = \"topic_dist_interactive\", # default name                        auto_open = FALSE) # TRUE enables output in browser visualize_topics(model = topic_model,                   filename = \"intertopic_distance_map\", # default name                  auto_open = FALSE) # TRUE enables output in browser visualize_heatmap(model = topic_model,                    filename = \"topics_similarity_heatmap\",                    auto_open = FALSE) visualize_hierarchy(model = topic_model,                      hierarchical_topics = NULL, # default                     filename = \"topic_hierarchy\", # default name, html extension                     auto_open = FALSE) # TRUE enables output in browser hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned) visualize_hierarchy(model = topic_model,                      hierarchical_topics = hierarchical_topics,                     filename = \"topic_hierarchy\", # default name, html extension                     auto_open = FALSE) # TRUE enables output in browser # Reduce dimensionality of embeddings using UMAP reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings) visualize_documents(model = topic_model,                      texts = texts_cleaned,                      reduced_embeddings = reduced_embeddings,                      filename = \"visualize_documents\", # default extension html                     auto_open = FALSE) # TRUE enables output in browser # Reduce dimensionality of embeddings using UMAP reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 3L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings) visualize_documents_3d(model = topic_model,                         texts = texts_cleaned,                         reduced_embeddings = reduced_embeddings,                         custom_labels = FALSE, # default                        hide_annotation = TRUE, # default                        tooltips = c(\"Topic\", \"Name\", \"Probability\", \"Text\"), # deault                        filename = \"visualize_documents_3d\", # default name                        auto_open = FALSE) # TRUE enables output in browser visualize_topics_over_time(model = topic_model,                             # see Topic Dynamics section above                            topics_over_time_model = topics_over_time,                            top_n_topics = 10, # default is 20                            filename = \"topics_over_time\") # default, html extension classes = as.list(dataset$genre) # text types topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes) visualize_topics_per_class(model = topic_model,                             topics_per_class = topics_per_class,                            start = 0, # default class                            end = 10, # default class                            filename = \"topics_per_class\", # default, html extension                             auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Document information","title":null,"text":"get_document_info_df() creates dataframe contains documents associated topics, characteristics keywords, probability scores, representative documents topic representation model results (e.g., keywords extracted KeyBERT, MMR, spacy, LLM descriptions topics).","code":"library(bertopicr) document_info_df <- get_document_info_df(model = topic_model,                                           texts = texts_cleaned,                                           drop_expanded_columns = TRUE) document_info_df |> head() |> rmarkdown::paged_table()"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Representative docs","title":null,"text":"First, create data frame similar df_docs , contains columns Topic, Document probs. use get_most_representative_docs() function extract representative documents chosen topic.","code":"# Create a data frame similar to df_docs df_docs <- tibble(Topic = results$Topic,                   Document = results$text_clean,                   probs = results$Probability) rep_docs <- get_most_representative_docs(df = df_docs,                                           topic_nr = 3,                                           n_docs = 5) unique(rep_docs) [1] \"fast alle ein mit sensoren vollgestopftes gerät mit sich herumtragen und ein immer größerer teil des lebens online stattfindet kann man sehr viel messen plötzlich existieren datenströme die auskunft geben über das fahrverhalten weil die bewegungsdaten des smartphones analysiert werden können datenströme von kameras die auf einer baustelle jeden einzelnen bauarbeiter überwachen und die jeweiligen leistungen sekundengenau messen die geschichte des kapitalismus ist eine geschichte der aggressiven effizienz und effizienz beginnt zwangsläufig mit der messung im digitalkapitalismus wird man darum kämpfen müssen welche der effizienzen gehoben werden sollen und welche nicht gehoben werden dürfen weil sie mit den\"                                                                          [2] \"die meisten menschen meinen dass digitalisierung ist wenn man per app seine seele dem teufel verkaufen oder auf ein fußballspiel in japan wetten kann so ähnlich geht es auch mit der künstlichen intelligenz sie ist ungefähr so segensreich wie die in den fünfzigerjahren prognostizierten atomgetriebenen staubsauger oder die küchen roboter aus mister spocks altenheim und selbstverständlich wird ki uns zunächst die supertolle lebenserleichterung ermöglichen dass der kühlschrank ding weiß wann schulferien sind und dass sie für ihre kinder eine zweiwöchige reise gebucht haben was dazu führt dass der kühlschrank eine woche vor und eine woche nach ostern etwas weniger fruchtjoghurt bestellt\"                                                                                                 [3] \"sollten sehr vorsichtig sein mit der künstlichen intelligenz wenn ich raten sollte was die größte bedrohung unserer existenz darstellt wäre es künstliche intelligenz bumm es war nicht das erste mal dass sich musk kritisch gegenüber dieser technologieform geäußert hat aber auch nicht das letzte mal denn mitte november schrieb musk in einem inzwischen gelöschten aber als echt bestätigten kommentar auf der debattenseite edge org die geschwindigkeit der entwicklung der künstlichen intelligenz ist unglaublich schnell innerhalb der nächsten fünf maximal zehn jahre könnte etwas ernsthaft gefährliches passieren musk befürchtet dass die menschheit die kontrolle verliert über die technologie\"                                                                                                 [4] \"kontrolliert wird ein digitaler algorithmen staat hätte ganz andere möglichkeiten er könnte die fahrgeschwindigkeit eines jeden autos in echtzeit kontrollieren und theoretisch bußgelder direkt von den konten der verkehrssünder abbuchen in der privatwirtschaft haben sich solche lückenlosen überwachungssysteme teils schon etabliert bei amazon feuert ein algorithmus manchmal mitarbeitende wenn diese aus sicht des unternehmens nicht gut genug performen die mittel der kontrolle und einflussnahme könnten durch algorithmen exponentiell wachsen manche die gesellschaft ordnenden prinzipien könnten sich dadurch umkehren im gesundheitssystem und im versicherungswesen etwa etabliert sich schon jetzt ein neues verständnis des vorsorgeprinzips derzeit zahlen wir oft monatlich geld in einen\" [5] \"sondern eine maschine erzeugt sie zeigen augenscheinlich in öl gemalte perserkatzen die entsetzt traurig oder wütend auf ihre smartphones starren offenbar weil sie mit ihren bitcoins so viel geld verloren haben es ist als mache eine künstliche intelligenz einen witz auf kosten der kryptowährungs fangemeinde die anweisung die das bild erzeugte hat sich ein silicon valley unternehmer ausgedacht er gehört zu den immer noch recht wenigen menschen die dalle solche anweisungen geben dürfen die stiftung openai schaltet nach und nach menschen frei die sich auf eine warteliste setzen lassen und angeben warum sie zugang zu ihrem werkzeug dalle haben möchten diese zugangsbeschränkungen sind\""},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic information","title":null,"text":"function get_topic_info_df() creates another useful data frame, extracted topics shows number associated documents (text chunks), topic id (Name), characeristic keywords according chosen representation models three (concatenated) representative documents.","code":"topic_info_df <- get_topic_info_df(model = topic_model,                                     drop_expanded_columns = TRUE) head(topic_info_df) |> rmarkdown::paged_table()"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Words in Topics","title":null,"text":"get_topics_df() function concentrates words associated certain topic probability scores. outliers (Topic = -1) usually included analysis. BERTopic offers function reduce number outliers update topic model.","code":"topics_df <- get_topics_df(model = topic_model) head(topics_df, 30) # A tibble: 30 × 3    Word          Score Topic    <chr>         <dbl> <dbl>  1 menschen    0.0149     -1  2 deutschland 0.0132     -1  3 deutschen   0.0101     -1  4 leben       0.00911    -1  5 welt        0.00864    -1  6 leute       0.00825    -1  7 gut         0.00799    -1  8 zeit        0.00799    -1  9 gerade      0.00780    -1 10 deutsche    0.00735    -1 # ℹ 20 more rows"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic Barchart","title":null,"text":"visualize_barchart() creates interactive barchart top five words frequently occurring topics.  might prefer create easy customizable barchart ggplot2 package using dataframe extracted get_topics_df() function run plotly library R ggplotly() function interactive barchart.","code":"visualize_barchart(model = topic_model,                     filename = \"topics_topwords_interactive_barchart\", # default                    open_file = FALSE) # TRUE enables output in browser library(ggplot2)  barchart <- topics_df |>    group_by(Topic) |>    filter(Topic >= 0 & Topic <= 8) |>    slice_head(n=5) |>    mutate(Topic = paste(\"Topic\", as.character(Topic)),           Word = reorder(Word, Score)) |>    ggplot(aes(Score, Word, fill = Topic)) +   geom_col() +   facet_wrap(~ Topic, scales = \"free\") +   theme(legend.position = \"none\")  library(plotly) ggplotly(barchart)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Find Topics","title":null,"text":"find_topics_df() function useful semantic search. can identify topics associated chosen query multiple queries.","code":"find_topics_df(model = topic_model,                 queries = \"migration\", # user input                top_n = 10, # default                return_tibble = TRUE) # default # A tibble: 10 × 3    keyword   topics similarity    <chr>      <int>      <dbl>  1 migration     17      0.563  2 migration     21      0.511  3 migration     -1      0.508  4 migration      0      0.484  5 migration      2      0.482  6 migration     11      0.481  7 migration     14      0.477  8 migration     12      0.477  9 migration     24      0.473 10 migration     13      0.472 find_topics_df(model = topic_model,                                 queries = c(\"migranten\", \"asylanten\"),                                top_n = 5) # A tibble: 10 × 3    keyword   topics similarity    <chr>      <int>      <dbl>  1 migranten     17      0.635  2 migranten     -1      0.551  3 migranten     12      0.549  4 migranten     21      0.543  5 migranten      2      0.528  6 asylanten     17      0.561  7 asylanten     24      0.526  8 asylanten     12      0.503  9 asylanten     21      0.500 10 asylanten     -1      0.498"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Get Topics","title":null,"text":"get_topic_df() function creates dataframe extracts top words chosen topic.","code":"get_topic_df(model = topic_model,                             topic_number = 0,                             top_n = 5, # default is 10                            return_tibble = TRUE) # default # A tibble: 5 × 3   Word       Score Topic   <chr>      <dbl> <int> 1 merkel    0.0467     0 2 spd       0.0409     0 3 kanzlerin 0.0285     0 4 partei    0.0272     0 5 cdu       0.0265     0"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic Distribution","title":null,"text":"visualize_distribution() function produces interactive barchart displays associated topics chosen document (text chunk). probability scores help identify likely topic(s) document (text chunk).","code":"# default filename: topic_dist_interactive.html visualize_distribution(model = topic_model,                         text_id = 1, # user input                        probabilities = probs, # see model training                        filename = \"topic_dist_interactive\", # default name                        auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Intertopic Distance Map","title":null,"text":"semantic relatedness topics can displayed visualize_topics() function.","code":"visualize_topics(model = topic_model,                   filename = \"intertopic_distance_map\", # default name                  auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic Similarity","title":null,"text":"can create similarity matrix applying cosine similarities generated topic embeddings. resulting matrix indicates similar topics . visualize similarity matrix can use visualize_heatmap() function.","code":"visualize_heatmap(model = topic_model,                    filename = \"topics_similarity_heatmap\",                    auto_open = FALSE)"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic hierarchy","title":null,"text":"best way display relatedness documents (text chunks) visualize_hierarchy() function, creates interactive dendrogram. additional option creation hierarchical topics list can included interactive dendrogram enables user identify joint expressions.","code":"visualize_hierarchy(model = topic_model,                      hierarchical_topics = NULL, # default                     filename = \"topic_hierarchy\", # default name, html extension                     auto_open = FALSE) # TRUE enables output in browser hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned) visualize_hierarchy(model = topic_model,                      hierarchical_topics = hierarchical_topics,                     filename = \"topic_hierarchy\", # default name, html extension                     auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Visualize Documents","title":null,"text":"visualize_documents() function displays identified clusters associated certain topic two dimensions. Usually, best reduce dimensionality embeddings UMAP (another dimension reduction method) produce intelligible visual results. interactive plot allows user select one clusters double-click mouse.  create interactive 3D plot, visualize_documents_3d function can used.","code":"# Reduce dimensionality of embeddings using UMAP reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings) visualize_documents(model = topic_model,                      texts = texts_cleaned,                      reduced_embeddings = reduced_embeddings,                      filename = \"visualize_documents\", # default extension html                     auto_open = FALSE) # TRUE enables output in browser # Reduce dimensionality of embeddings using UMAP reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 3L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings) visualize_documents_3d(model = topic_model,                         texts = texts_cleaned,                         reduced_embeddings = reduced_embeddings,                         custom_labels = FALSE, # default                        hide_annotation = TRUE, # default                        tooltips = c(\"Topic\", \"Name\", \"Probability\", \"Text\"), # deault                        filename = \"visualize_documents_3d\", # default name                        auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Topic Development","title":null,"text":"can also inspect chosen number topics develop certain period time. visualize_topics_over_time() function assumes timestamps, topic model topics time model already defined (e.g., model preparation step topic model training). timestamps need integers certain date format (see model preparation step ).","code":"visualize_topics_over_time(model = topic_model,                             # see Topic Dynamics section above                            topics_over_time_model = topics_over_time,                            top_n_topics = 10, # default is 20                            filename = \"topics_over_time\") # default, html extension"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Groups","title":null,"text":"dataset includes categorical variables (groups, classes, etc.), can use visualize_topics_per_class() function display interactive barchart groups classes associated chosen topic. double-click mouse, user can choose single topic inspect frequency groups.","code":"classes = as.list(dataset$genre) # text types topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes) visualize_topics_per_class(model = topic_model,                             topics_per_class = topics_per_class,                            start = 0, # default class                            end = 10, # default class                            filename = \"topics_per_class\", # default, html extension                             auto_open = FALSE) # TRUE enables output in browser"},{"path":"https://tpetric7.github.io/bertopicr/topics__spiegel.html","id":null,"dir":"","previous_headings":"","what":"Conclusion","title":null,"text":"BERTopic awesome topic modeling package Python. bertopicr package tries bring functionalities R programming environment magnificent reticulate package interface Python backend. BERTopic offers number additional functions, included subsequent versions bertopicr.","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-036","dir":"Changelog","previous_headings":"","what":"bertopicr 0.3.6","title":"bertopicr 0.3.6","text":"Documentation & Vignette fixes bump version 0.3.6","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-035","dir":"Changelog","previous_headings":"","what":"bertopicr 0.3.5","title":"bertopicr 0.3.5","text":"Added setup_python_environment() helper reticulate setup. Added train_bertopic_model() helper end--end model training. Added save_bertopic_model() load_bertopic_model() persist reload models extras. Added vignettes train_and_save_model.Rmd load_and_reuse_model.Rmd. Added demo script inst/scripts/train_model_function_demo.R.","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-034","dir":"Changelog","previous_headings":"","what":"bertopicr 0.3.4","title":"bertopicr 0.3.4","text":"visualize_documents_2d() function richer labeling options added already existing ones document visualization (visualize_documents() visualize_documents_3d()). visualize_documents_3d() function also includes improved labeling options previous versions.","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-033","dir":"Changelog","previous_headings":"","what":"bertopicr 0.3.3","title":"bertopicr 0.3.3","text":"update Python package BERTopic version 0.17.4 dependencies introduced conflicts certain R packages. Avoid attaching R libraries like arrow plotly workflow. Due update Python package BERTopic dependencies, recommended load Python packages BERTopic sentence-transformers certain R libraries, e.g., readr_rds(). README.md file rewritten easier understanding installation options requirements. vignette added (alongside Quarto tutorial pre-computed HTML file). Setup pkgdown website Github (https://tpetric7.github.io/bertopicr) another website Netlify (https://bertopicr.netlify.app/).","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-032","dir":"Changelog","previous_headings":"","what":"bertopicr 0.3.2","title":"bertopicr 0.3.2","text":"update Python package BERTopic version 0.17.0, might update packages well (like ) create new virtual environment compatible packages. updating BERTopic=0.17.0, might experience visualize_documents() function doesn’t render dots scatterplot. simple temporary fix open Python file _documents.py visualize_documents() function ofBERTopic (Windows system sits anaconda3\\envs\\bertopic\\Lib\\site-packages\\bertopic\\plotting\\) change go.Scattergl go.Scatter fig.add_trace() function (occurs twice Python script).","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-031","dir":"Changelog","previous_headings":"","what":"bertopicr 0.3.1","title":"bertopicr 0.3.1","text":"Added .onload() function allow MacOS users import BERTopic installed Python modules virtual environment.","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-030","dir":"Changelog","previous_headings":"","what":"bertopicr 0.3.0","title":"bertopicr 0.3.0","text":"R package bertopicr introduces training visualization helpers reproduce functions Python BERTopic package. easier memorization, functions names (nearly) identical Python package BERTopic.","code":""},{"path":"https://tpetric7.github.io/bertopicr/news/index.html","id":"bertopicr-02","dir":"Changelog","previous_headings":"","what":"bertopicr 0.2","title":"bertopicr 0.2","text":"preliminary collection R wrapping functions Python BERTopic package tutorial R users.","code":""}]
