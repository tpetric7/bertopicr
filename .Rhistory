install.packages("rlang")
install.packages("readr")
setwd("C:/Users/teodo/Documents/bertopicr")
library(usethis)
library(roxygen2)
roxygenize()
library(usethis)
library(roxygen2)
setwd("C:/Users/teodo/Documents/bertopicr")
roxygenize()
packageVersion("rlang")
packageVersion("readr")
install.packages("rlang", dependencies = TRUE)
install.packages("readr", dependencies = TRUE)
library(usethis)
library(roxygen2)
setwd("C:/Users/teodo/Documents/bertopicr")
roxygenize()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
roxygenize()
devtools::check()
Sys.setenv(R_KEEP_PKG_SOURCE = "yes")
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
devtools::load_all()
library(tidyverse)
library(tidytext)
library(quanteda)
library(tictoc)
library(htmltools)
library(htmlwidgets)
library(arrow)
library(reticulate)
use_python("c:/Users/teodo/anaconda3/envs/bertopic", required = TRUE)
reticulate::py_config()
reticulate::py_available()
library(bertopicr)
# input_file <- "spiegel_poldeu2.csv"
inpt_folder <- "c:/Users/teodo/Documents/R/bertopic-r/data"
input_file <- "spiegel_comments_column_selection.parquet"
dataset <- read_parquet(file.path(input_folder, input_file))
# input_file <- "spiegel_poldeu2.csv"
input_folder <- "c:/Users/teodo/Documents/R/bertopic-r/data"
input_file <- "spiegel_comments_column_selection.parquet"
dataset <- read_parquet(file.path(input_folder, input_file))
names(df)
dim(df)
# input_file <- "spiegel_poldeu2.csv"
input_folder <- "c:/Users/teodo/Documents/R/bertopic-r/data"
input_file <- "spiegel_comments_column_selection.parquet"
dataset <- read_parquet(file.path(input_folder, input_file))
names(dataset)
dim(dataset)
input_folder <- "c:/Users/teodo/Documents/R/bertopic-r/stopwords"
input_file <- "stopwords/all_stopwords.txt"
all_stopwords <- read_lines(file.path(input_folder, input_file))
input_folder <- "c:/Users/teodo/Documents/R/bertopic-r/stopwords"
input_file <- "all_stopwords.txt"
all_stopwords <- read_lines(file.path(input_folder, input_file))
texts_cleaned = dataset$text_clean
titles = dataset$doc_id
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
texts_cleaned[[1]]
#| eval: true
# Import necessary Python modules
np <- import("numpy")
umap <- import("umap")
UMAP <- umap$UMAP
hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN
sklearn <- import("sklearn")
CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer
bertopic <- import("bertopic")
# Embed the sentences
py <- import_builtins()
sentence_transformers <- import("sentence_transformers")
SentenceTransformer <- sentence_transformers$SentenceTransformer
embedding_model = SentenceTransformer("BAAI/bge-m3")
embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)
# Initialize UMAP and HDBSCAN models
umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)
hdbscan_model <- HDBSCAN(min_cluster_size=50L, min_samples = 20L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)
# Initialize CountVectorizer
vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),
max_features = 10000L, max_df = 50L,
stop_words = all_stopwords)
sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned)
sentence_vectors_dense <- np$array(sentence_vectors)
sentence_vectors_dense <- py_to_r(sentence_vectors_dense)
# Initialize representation models
keybert_model <- bertopic$representation$KeyBERTInspired()
openai <- import("openai")
OpenAI <- openai$OpenAI
ollama <- import("ollama")
# Point to the local server (ollama or lm-studio)
client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama')
# client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')
prompt <- "
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]
Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
topic: <topic label>
"
# download an appropriate local LLM for your computer with ollama/lm-studio
openai_model <- bertopic$representation$OpenAI(client,
model = "llama3.1:8b-instruct-fp16",
exponential_backoff = TRUE,
chat = TRUE,
prompt = prompt)
# downlaod a spacy language model from spacy.io
pos_model <- bertopic$representation$PartOfSpeech("de_core_news_lg")
# diversity set relatively high to reduce repetition of keyword word forms
mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)
# Combine all representation models
representation_model <- list(
"KeyBERT" = keybert_model,
"OpenAI" = openai_model,
"MMR" = mmr_model,
"POS" = pos_model
)
# We define a number of topics that we know are in the documents
zeroshot_topic_list  <- list("german national identity", "minority issues in germany")
# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
embedding_model = embedding_model,
umap_model = umap_model,
hdbscan_model = hdbscan_model,
vectorizer_model = vectorizer_model,
# zeroshot_topic_list = zeroshot_topic_list,
# zeroshot_min_similarity = 0.85,
representation_model = representation_model,
calculate_probabilities = TRUE,
top_n_words = 10L,
verbose = TRUE
)
tictoc::tic()
# Fit the model and transform the texts
fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# probs <- fit_transform[[2]]
# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities
tictoc::toc()
# Converting R Date to Python datetime
datetime <- import("datetime")
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
# Convert each R date object to an ISO 8601 string
timestamps <- lapply(timestamps, function(x) {
format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
})
# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)
# Combine results with additional columns
results <- dataset |>
mutate(Topic = topics,
Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence
results <- results |>
mutate(row_id = row_number()) |>
select(row_id, everything())
head(results,100) |> rmarkdown::paged_table()
# results |>
#   saveRDS("data/spiegel_kommentare_topic_results_df_02.rds")
# results |>
#   write_csv("data/spiegel_kommentare_topic_results_df_02.csv")
# results |>
#   writexl::write_xlsx("data/spiegel_kommentare_topic_results_df_02.xlsx")
library(bertopicr)
library(bertopicr)
document_info_df <- get_document_info_df(topic_model, texts = texts_cleaned, drop_expanded_columns = TRUE)
document_info_df |> head()
topic_info_df <- get_topic_info_df(topic_model, drop_expanded_columns = TRUE)
head(topic_info_df)
topics_df <- get_topics_df(topic_model)
head(topics_df)
head(topics_df, 10)
visualize_barchart(topic_model)
topic_df_0 <- get_topic_df(topic_model, topic_number = 0, top_n = 5)
topic_df_0
visualize_distribution(topic_model, text_id = 1, probabilities = probs)
visualize_topics(topic_model)
visualize_hierarchy(topic_model)
visualize_hierarchy(model = topic_model)
visualize_documents(topic_model, texts = texts_cleaned, reduced_embeddings = embeddings)
find_topics_df(topic_model, queries = "migration")
visualize_topics_over_time(topic_model,
topics_over_time_model = topics_over_time,
top_n_topics = 10)
find_topics_df(topic_model, queries = "migration")
topic_df_0 <- get_topic_df(topic_model, topic_number = 0, top_n = 5)
topic_df_0
visualize_distribution(topic_model, text_id = 1, probabilities = probs)
visualize_topics(topic_model)
visualize_hierarchy(model = topic_model)
visualize_documents(topic_model, texts = texts_cleaned, reduced_embeddings = embeddings)
visualize_topics_over_time(topic_model,
topics_over_time_model = topics_over_time,
top_n_topics = 10)
visualize_topics_over_time(model = topic_model,
topics_over_time_model = topics_over_time,
top_n_topics = 10)
classes = as.list(dataset$text_type) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)
visualize_topics_per_class(model = topic_model, topics_per_class = topics_per_class)
visualize_topics_per_class(model = topic_model, topics_per_class = topics_per_class, auto_open = FALSE)
visualize_topics_over_time(model = topic_model,
topics_over_time_model = topics_over_time,
top_n_topics = 20)
# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)
visualize_documents(topic_model, texts = texts_cleaned, reduced_embeddings = reduced_embeddings)
library(usethis)
library(roxygen2)
getwd("C:/Users/teodo/Documents/bertopicr")
setwd("C:/Users/teodo/Documents/bertopicr")
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
devtools::load_all()
library(tidyverse)
library(tidytext)
library(quanteda)
library(tictoc)
library(htmltools)
library(htmlwidgets)
library(arrow)
library(reticulate)
use_python("c:/Users/teodo/anaconda3/envs/bertopic", required = TRUE)
reticulate::py_config()
reticulate::py_available()
library(bertopicr)
# input_file <- "spiegel_poldeu2.csv"
input_folder <- "c:/Users/teodo/Documents/R/bertopic-r/data"
input_file <- "spiegel_comments_column_selection.parquet"
dataset <- read_parquet(file.path(input_folder, input_file))
names(dataset)
dim(dataset)
input_folder <- "c:/Users/teodo/Documents/R/bertopic-r/stopwords"
input_file <- "all_stopwords.txt"
all_stopwords <- read_lines(file.path(input_folder, input_file))
texts_cleaned = dataset$text_clean
titles = dataset$doc_id
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
texts_cleaned[[1]]
#| eval: true
# Import necessary Python modules
np <- import("numpy")
umap <- import("umap")
UMAP <- umap$UMAP
hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN
sklearn <- import("sklearn")
CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer
bertopic <- import("bertopic")
# Embed the sentences
py <- import_builtins()
sentence_transformers <- import("sentence_transformers")
SentenceTransformer <- sentence_transformers$SentenceTransformer
embedding_model = SentenceTransformer("BAAI/bge-m3")
embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)
# Initialize UMAP and HDBSCAN models
umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)
hdbscan_model <- HDBSCAN(min_cluster_size=50L, min_samples = 20L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)
# Initialize CountVectorizer
vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),
max_features = 10000L, max_df = 50L,
stop_words = all_stopwords)
sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned)
sentence_vectors_dense <- np$array(sentence_vectors)
sentence_vectors_dense <- py_to_r(sentence_vectors_dense)
# Initialize representation models
keybert_model <- bertopic$representation$KeyBERTInspired()
openai <- import("openai")
OpenAI <- openai$OpenAI
ollama <- import("ollama")
# Point to the local server (ollama or lm-studio)
client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama')
# client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')
prompt <- "
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]
Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
topic: <topic label>
"
# download an appropriate local LLM for your computer with ollama/lm-studio
openai_model <- bertopic$representation$OpenAI(client,
model = "llama3.1:8b-instruct-fp16",
exponential_backoff = TRUE,
chat = TRUE,
prompt = prompt)
# downlaod a spacy language model from spacy.io
pos_model <- bertopic$representation$PartOfSpeech("de_core_news_lg")
# diversity set relatively high to reduce repetition of keyword word forms
mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)
# Combine all representation models
representation_model <- list(
"KeyBERT" = keybert_model,
"OpenAI" = openai_model,
"MMR" = mmr_model,
"POS" = pos_model
)
# We define a number of topics that we know are in the documents
zeroshot_topic_list  <- list("german national identity", "minority issues in germany")
# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
embedding_model = embedding_model,
umap_model = umap_model,
hdbscan_model = hdbscan_model,
vectorizer_model = vectorizer_model,
# zeroshot_topic_list = zeroshot_topic_list,
# zeroshot_min_similarity = 0.85,
representation_model = representation_model,
calculate_probabilities = TRUE,
top_n_words = 10L,
verbose = TRUE
)
tictoc::tic()
# Fit the model and transform the texts
fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# probs <- fit_transform[[2]]
# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities
tictoc::toc()
# Converting R Date to Python datetime
datetime <- import("datetime")
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
# Convert each R date object to an ISO 8601 string
timestamps <- lapply(timestamps, function(x) {
format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
})
# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)
# Combine results with additional columns
results <- dataset |>
mutate(Topic = topics,
Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence
results <- results |>
mutate(row_id = row_number()) |>
select(row_id, everything())
head(results,100) |> rmarkdown::paged_table()
# results |>
#   saveRDS("data/spiegel_kommentare_topic_results_df_02.rds")
# results |>
#   write_csv("data/spiegel_kommentare_topic_results_df_02.csv")
# results |>
#   writexl::write_xlsx("data/spiegel_kommentare_topic_results_df_02.xlsx")
library(bertopicr)
document_info_df <- get_document_info_df(topic_model, texts = texts_cleaned, drop_expanded_columns = TRUE)
document_info_df |> head()
topic_info_df <- get_topic_info_df(topic_model, drop_expanded_columns = TRUE)
head(topic_info_df)
topics_df <- get_topics_df(topic_model)
head(topics_df, 10)
visualize_barchart(topic_model)
find_topics_df(topic_model, queries = "migration")
topic_df_0 <- get_topic_df(topic_model, topic_number = 0, top_n = 5)
topic_df_0
visualize_distribution(topic_model, text_id = 1, probabilities = probs)
visualize_topics(topic_model)
visualize_hierarchy(model = topic_model)
# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)
visualize_documents(topic_model, texts = texts_cleaned, reduced_embeddings = reduced_embeddings)
visualize_documents(topic_model, reduced_embeddings = reduced_embeddings)
visualize_documents(topic_model, texts = texts_cleaned, reduced_embeddings = reduced_embeddings, custom_labels = FALSE, hide_annotation = TRUE)
visualize_topics_over_time(model = topic_model,
topics_over_time_model = topics_over_time,
top_n_topics = 20)
visualize_topics_over_time(model = topic_model,
topics_over_time_model = topics_over_time,
top_n_topics = 5)
hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned)
visualize_hierarchy(model = topic_model,
hierarchical_topics = hierarchical_topics, # optional
filename = "topic_hierarchy",
auto_open = FALSE)
# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)
visualize_documents(topic_model, texts = texts_cleaned, reduced_embeddings = reduced_embeddings, custom_labels = FALSE, hide_annotation = TRUE)
