mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)
# Combine all representation models
representation_model <- list(
"KeyBERT" = keybert_model,
"OpenAI" = openai_model,
"MMR" = mmr_model,
"POS" = pos_model
)
# We can define a number of topics that we know are in the documents
zeroshot_topic_list  <- list("german national identity", "minority issues in germany")
# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
embedding_model = embedding_model,
umap_model = umap_model,
hdbscan_model = hdbscan_model,
vectorizer_model = vectorizer_model,
# zeroshot_topic_list = zeroshot_topic_list,
# zeroshot_min_similarity = 0.85,
representation_model = representation_model,
calculate_probabilities = TRUE,
top_n_words = 10L,
verbose = TRUE
)
tictoc::tic()
# Fit the model and transform the texts
fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# probs <- fit_transform[[2]]
# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities
tictoc::toc()
# Converting R Date to Python datetime
datetime <- import("datetime")
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
# Convert each R date object to an ISO 8601 string
timestamps <- lapply(timestamps, function(x) {
format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
})
# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)
# Combine results with additional columns
results <- dataset |>
mutate(Topic = topics,
Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence
results <- results |>
mutate(row_id = row_number()) |>
select(row_id, everything())
head(results,10) |> rmarkdown::paged_table()
library(bertopicr)
document_info_df <- get_document_info_df(model = topic_model,
texts = texts_cleaned,
drop_expanded_columns = TRUE)
document_info_df |> head() |> rmarkdown::paged_table()
# Create a data frame similar to df_docs
df_docs <- tibble(Topic = results$Topic,
Document = results$text_clean,
probs = results$Probability)
rep_docs <- get_most_representative_docs(df = df_docs,
topic_nr = 3,
n_docs = 5)
unique(rep_docs)
topic_info_df <- get_topic_info_df(model = topic_model,
drop_expanded_columns = TRUE)
head(topic_info_df) |> rmarkdown::paged_table()
topics_df <- get_topics_df(model = topic_model)
head(topics_df, 10)
visualize_barchart(model = topic_model,
filename = "topics_topwords_interactive_barchart.html", # default
open_file = FALSE) # TRUE enables output in browser
find_topics_df(model = topic_model,
queries = "migration", # user input
top_n = 10, # default
return_tibble = TRUE) # default
find_topics_df(model = topic_model,
queries = c("migranten", "asylanten"),
top_n = 5)
get_topic_df(model = topic_model,
topic_number = 0,
top_n = 5, # default is 10
return_tibble = TRUE) # default
# default filename: topic_dist_interactive.html
visualize_distribution(model = topic_model,
text_id = 1, # user input
probabilities = probs) # see model training
visualize_topics(model = topic_model,
filename = "intertopic_distance_map") # default name
library(dplyr)
library(tidyr)
library(purrr)
library(utils)
library(tibble)
library(readr)
library(tictoc)
library(htmltools)
library(arrow)
# interface with Python
library(reticulate)
use_python("c:/Users/teodo/anaconda3/envs/bertopic", required = TRUE)
reticulate::py_config()
reticulate::py_available()
library(bertopicr)
# input_file <- "spiegel_poldeu2.csv"
input_folder <- "data"
input_file <- "spiegel_sample.parquet"
dataset <- read_parquet(file.path(input_folder, input_file))
names(dataset)
dim(dataset)
input_folder <- "data"
input_file <- "all_stopwords.txt"
all_stopwords <- read_lines(file.path(input_folder, input_file))
texts_cleaned = dataset$text_clean
titles = dataset$doc_id
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
texts_cleaned[[1]]
# Import necessary Python modules
np <- import("numpy")
umap <- import("umap")
UMAP <- umap$UMAP
hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN
sklearn <- import("sklearn")
CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer
bertopic <- import("bertopic")
plotly <- import("plotly")
datetime <- import("datetime")
# Embed the sentences
py <- import_builtins()
sentence_transformers <- import("sentence_transformers")
SentenceTransformer <- sentence_transformers$SentenceTransformer
embedding_model = SentenceTransformer("BAAI/bge-m3")
embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)
# Initialize UMAP and HDBSCAN models
umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)
hdbscan_model <- HDBSCAN(min_cluster_size=15L, min_samples = 10L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)
# Initialize CountVectorizer
vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),
max_features = 10000L, max_df = 50L,
stop_words = all_stopwords)
sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned)
sentence_vectors_dense <- np$array(sentence_vectors)
sentence_vectors_dense <- py_to_r(sentence_vectors_dense)
# Initialize representation models
keybert_model <- bertopic$representation$KeyBERTInspired()
openai <- import("openai")
OpenAI <- openai$OpenAI
ollama <- import("ollama")
# Point to the local server (ollama or lm-studio)
client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama')
# client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')
prompt <- "
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]
Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
topic: <topic label>
"
# download an appropriate local LLM for your computer with ollama/lm-studio
openai_model <- bertopic$representation$OpenAI(client,
model = "llama3.1:8b-instruct-fp16",
exponential_backoff = TRUE,
chat = TRUE,
prompt = prompt)
# downlaod a spacy language model from spacy.io before use here
pos_model <- bertopic$representation$PartOfSpeech("de_core_news_lg")
# diversity set relatively high to reduce repetition of keyword word forms
mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)
# Combine all representation models
representation_model <- list(
"KeyBERT" = keybert_model,
"OpenAI" = openai_model,
"MMR" = mmr_model,
"POS" = pos_model
)
# We can define a number of topics that we know are in the documents
zeroshot_topic_list  <- list("german national identity", "minority issues in germany")
# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
embedding_model = embedding_model,
umap_model = umap_model,
hdbscan_model = hdbscan_model,
vectorizer_model = vectorizer_model,
# zeroshot_topic_list = zeroshot_topic_list,
# zeroshot_min_similarity = 0.85,
representation_model = representation_model,
calculate_probabilities = TRUE,
top_n_words = 10L,
verbose = TRUE
)
tictoc::tic()
# Fit the model and transform the texts
fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# probs <- fit_transform[[2]]
# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities
tictoc::toc()
# Converting R Date to Python datetime
datetime <- import("datetime")
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
# Convert each R date object to an ISO 8601 string
timestamps <- lapply(timestamps, function(x) {
format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
})
# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)
# Combine results with additional columns
results <- dataset |>
mutate(Topic = topics,
Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence
results <- results |>
mutate(row_id = row_number()) |>
select(row_id, everything())
head(results,10) |> rmarkdown::paged_table()
library(bertopicr)
document_info_df <- get_document_info_df(model = topic_model,
texts = texts_cleaned,
drop_expanded_columns = TRUE)
document_info_df |> head() |> rmarkdown::paged_table()
# Create a data frame similar to df_docs
df_docs <- tibble(Topic = results$Topic,
Document = results$text_clean,
probs = results$Probability)
rep_docs <- get_most_representative_docs(df = df_docs,
topic_nr = 3,
n_docs = 5)
unique(rep_docs)
topic_info_df <- get_topic_info_df(model = topic_model,
drop_expanded_columns = TRUE)
head(topic_info_df) |> rmarkdown::paged_table()
topics_df <- get_topics_df(model = topic_model)
head(topics_df, 10)
visualize_barchart(model = topic_model,
filename = "topics_topwords_interactive_barchart.html", # default
open_file = FALSE) # TRUE enables output in browser
find_topics_df(model = topic_model,
queries = "migration", # user input
top_n = 10, # default
return_tibble = TRUE) # default
find_topics_df(model = topic_model,
queries = c("migranten", "asylanten"),
top_n = 5)
get_topic_df(model = topic_model,
topic_number = 0,
top_n = 5, # default is 10
return_tibble = TRUE) # default
# default filename: topic_dist_interactive.html
visualize_distribution(model = topic_model,
text_id = 1, # user input
probabilities = probs) # see model training
visualize_topics(model = topic_model,
filename = "intertopic_distance_map") # default name
visualize_hierarchy(model = topic_model,
hierarchical_topics = NULL, # default
filename = "topic_hierarchy", # default name, html extension
auto_open = FALSE) # TRUE enables output in browser
hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned)
visualize_hierarchy(model = topic_model,
hierarchical_topics = hierarchical_topics,
filename = "topic_hierarchy", # default name, html extension
auto_open = FALSE) # TRUE enables output in browser
# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)
visualize_documents(model = topic_model,
texts = texts_cleaned,
reduced_embeddings = reduced_embeddings,
filename = "visualize_documents", # default extension html
auto_open = FALSE) # TRUE enables output in browser
visualize_topics_over_time(model = topic_model,
# see Topic Dynamics section above
topics_over_time_model = topics_over_time,
top_n_topics = 10, # default is 20
filename = "topics_over_time") # default, html extension
classes = as.list(dataset$genre) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)
visualize_topics_per_class(model = topic_model,
topics_per_class = topics_per_class,
start = 0, # default
end = 10, # default
filename = "topics_per_class", # default, html extension
auto_open = FALSE) # TRUE enables output in browser
classes = as.list(dataset$genre) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)
visualize_topics_per_class(model = topic_model,
topics_per_class = topics_per_class,
start = 0, # default
end = 9, # default
filename = "topics_per_class", # default, html extension
auto_open = FALSE) # TRUE enables output in browser
# input_file <- "spiegel_poldeu2.csv"
input_folder <- "data"
input_file <- "spiegel_sample.parquet"
input_file <- "spiegel_comments_column_selection.parquet"
dataset <- read_parquet(file.path(input_folder, input_file)) |>
slice_sample(n = 3000)
write_parquet(dataset, "data/spiegel_sample.parquet")
names(dataset)
dim(dataset)
input_folder <- "data"
input_file <- "all_stopwords.txt"
all_stopwords <- read_lines(file.path(input_folder, input_file))
texts_cleaned = dataset$text_clean
titles = dataset$doc_id
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
texts_cleaned[[1]]
# Import necessary Python modules
np <- import("numpy")
umap <- import("umap")
UMAP <- umap$UMAP
hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN
sklearn <- import("sklearn")
CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer
bertopic <- import("bertopic")
plotly <- import("plotly")
datetime <- import("datetime")
# Embed the sentences
py <- import_builtins()
sentence_transformers <- import("sentence_transformers")
SentenceTransformer <- sentence_transformers$SentenceTransformer
embedding_model = SentenceTransformer("BAAI/bge-m3")
embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)
# Initialize UMAP and HDBSCAN models
umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)
hdbscan_model <- HDBSCAN(min_cluster_size=15L, min_samples = 10L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)
# Initialize CountVectorizer
vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L),
max_features = 10000L, max_df = 50L,
stop_words = all_stopwords)
sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned)
sentence_vectors_dense <- np$array(sentence_vectors)
sentence_vectors_dense <- py_to_r(sentence_vectors_dense)
# Initialize representation models
keybert_model <- bertopic$representation$KeyBERTInspired()
openai <- import("openai")
OpenAI <- openai$OpenAI
ollama <- import("ollama")
# Point to the local server (ollama or lm-studio)
client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama')
# client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')
prompt <- "
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]
Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
topic: <topic label>
"
# download an appropriate local LLM for your computer with ollama/lm-studio
openai_model <- bertopic$representation$OpenAI(client,
model = "llama3.1:8b-instruct-fp16",
exponential_backoff = TRUE,
chat = TRUE,
prompt = prompt)
# downlaod a spacy language model from spacy.io before use here
pos_model <- bertopic$representation$PartOfSpeech("de_core_news_lg")
# diversity set relatively high to reduce repetition of keyword word forms
mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)
# Combine all representation models
representation_model <- list(
"KeyBERT" = keybert_model,
"OpenAI" = openai_model,
"MMR" = mmr_model,
"POS" = pos_model
)
# We can define a number of topics that we know are in the documents
zeroshot_topic_list  <- list("german national identity", "minority issues in germany")
# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
embedding_model = embedding_model,
umap_model = umap_model,
hdbscan_model = hdbscan_model,
vectorizer_model = vectorizer_model,
# zeroshot_topic_list = zeroshot_topic_list,
# zeroshot_min_similarity = 0.85,
representation_model = representation_model,
calculate_probabilities = TRUE,
top_n_words = 10L,
verbose = TRUE
)
tictoc::tic()
# Fit the model and transform the texts
fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# probs <- fit_transform[[2]]
# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities
tictoc::toc()
# Converting R Date to Python datetime
datetime <- import("datetime")
timestamps <- as.list(dataset$date)
# timestamps <- as.integer(dataset$year)
# Convert each R date object to an ISO 8601 string
timestamps <- lapply(timestamps, function(x) {
format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
})
# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)
# Combine results with additional columns
results <- dataset |>
mutate(Topic = topics,
Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence
results <- results |>
mutate(row_id = row_number()) |>
select(row_id, everything())
head(results,10) |> rmarkdown::paged_table()
library(bertopicr)
document_info_df <- get_document_info_df(model = topic_model,
texts = texts_cleaned,
drop_expanded_columns = TRUE)
document_info_df |> head() |> rmarkdown::paged_table()
# Create a data frame similar to df_docs
df_docs <- tibble(Topic = results$Topic,
Document = results$text_clean,
probs = results$Probability)
rep_docs <- get_most_representative_docs(df = df_docs,
topic_nr = 3,
n_docs = 5)
unique(rep_docs)
topic_info_df <- get_topic_info_df(model = topic_model,
drop_expanded_columns = TRUE)
head(topic_info_df) |> rmarkdown::paged_table()
topics_df <- get_topics_df(model = topic_model)
head(topics_df, 10)
visualize_barchart(model = topic_model,
filename = "topics_topwords_interactive_barchart.html", # default
open_file = FALSE) # TRUE enables output in browser
find_topics_df(model = topic_model,
queries = "migration", # user input
top_n = 10, # default
return_tibble = TRUE) # default
find_topics_df(model = topic_model,
queries = c("migranten", "asylanten"),
top_n = 5)
get_topic_df(model = topic_model,
topic_number = 0,
top_n = 5, # default is 10
return_tibble = TRUE) # default
# default filename: topic_dist_interactive.html
visualize_distribution(model = topic_model,
text_id = 1, # user input
probabilities = probs) # see model training
visualize_topics(model = topic_model,
filename = "intertopic_distance_map") # default name
visualize_hierarchy(model = topic_model,
hierarchical_topics = NULL, # default
filename = "topic_hierarchy", # default name, html extension
auto_open = FALSE) # TRUE enables output in browser
hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned)
visualize_hierarchy(model = topic_model,
hierarchical_topics = hierarchical_topics,
filename = "topic_hierarchy", # default name, html extension
auto_open = FALSE) # TRUE enables output in browser
# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)
visualize_documents(model = topic_model,
texts = texts_cleaned,
reduced_embeddings = reduced_embeddings,
filename = "visualize_documents", # default extension html
auto_open = FALSE) # TRUE enables output in browser
visualize_topics_over_time(model = topic_model,
# see Topic Dynamics section above
topics_over_time_model = topics_over_time,
top_n_topics = 10, # default is 20
filename = "topics_over_time") # default, html extension
classes = as.list(dataset$genre) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)
visualize_topics_per_class(model = topic_model,
topics_per_class = topics_per_class,
start = 0, # default
end = 9, # default
filename = "topics_per_class", # default, html extension
auto_open = FALSE) # TRUE enables output in browser
classes = as.list(dataset$genre) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)
visualize_topics_per_class(model = topic_model,
topics_per_class = topics_per_class,
start = 0, # default
end = 10, # default
filename = "topics_per_class", # default, html extension
auto_open = FALSE) # TRUE enables output in browser
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
quarto check()
quarto::quarto check()
installed.packages("quarto")
installed.packages("quanteda")
install.packages("quarto")
quarto::quarto_version()
devtools::build_vignettes()
quarto::quarto check
devtools::build_vignettes()
devtools::build_vignettes(quiet = FALSE)
library(usethis)
library(roxygen2)
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
roxygenize()
devtools::check()
