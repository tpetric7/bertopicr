df |> filter(str_detect(text_clean, "mssen|knnen")) |> pull(text_clean)
training_results <- train_bertopic_model(docs = docs, embedding_model = "Qwen/Qwen3-Embedding-0.6B", embedding_show_progress = TRUE)
library(tidyverse)
library(reticulate)
library(bertopicr)
source("R/train_bertopic_model.R")
reticulate::virtualenv_python(envname = "c:/Users/teodo/Documents/.virtualenvs/rbtopic")
reticulate::use_virtualenv(envname = "c:/Users/teodo/Documents/.virtualenvs/rbtopic")
reticulate::use_virtualenv("c:/Users/teodo/Documents/.virtualenvs/rbtopic")
reticulate::use_virtualenv("c:/Users/teodo/Documents/.virtualenvs/rbtopic", required = TRUE)
reticulate::use_python("c:/Users/teodo/Documents/.virtualenvs/rbtopic", required = TRUE)
reticulate::use_python("c:/Users/teodo/Documents/.virtualenvs/rbtopic/", required = TRUE)
reticulate::use_python("c:/Users/teodo/Documents/.virtualenvs/rbtopic/Scripts", required = TRUE)
reticulate::use_virtualenv("c:/Users/teodo/Documents/.virtualenvs/rbtopic/Scripts", required = TRUE)
reticulate::use_virtualenv("c:/Users/teodo/Documents/.virtualenvs/rbtopic/", required = TRUE)
library(tidyverse)
library(reticulate)
library(bertopicr)
source("R/train_bertopic_model.R")
reticulate::use_virtualenv("c:/Users/teodo/Documents/.virtualenvs/rbtopic/", required = TRUE)
reticulate::py_config()
reticulate::py_available()
library(tidyverse)
library(reticulate)
library(bertopicr)
source("R/train_bertopic_model.R")
# reticulate::use_virtualenv("c:/Users/teodo/Documents/.virtualenvs/rbtopic/", required = TRUE)
reticulate::use_virtualenv("c:/Users/teodo/Documents/R/bertopic/bertopic4r/", required = TRUE)
reticulate::py_config()
reticulate::py_available()
sample <- "C:/Users/teodo/Documents/bertopic4r/inst/extdata/spiegel_sample.rds"
all_stopwords <- read_lines("C:/Users/teodo/Documents/bertopic4r/inst/extdata/all_stopwords.txt")
df <- read_rds(sample)
# df <- df |> head(1000)
docs <- df |> pull(text_clean)
topic_model <- train_bertopic_model(docs = docs, embedding_model = "Qwen/Qwen3-Embedding-0.6B", embedding_show_progress = TRUE, ollama_model = "gpt-oss:20b")
(doc_info <- get_document_info_df(model = topic_model$model, texts = docs))
(topic_info <- get_topic_info_df(topic_model$model))
get_most_representative_docs(doc_info |> rename(probs = Probability), topic_nr = 1, n_docs = 2)
get_topic_df(topic_model$model, topic_number = 1)
visualize_barchart(model = topic_model$model, filename = "barchart_demo")
visualize_distribution(topic_model$model, text_id = 1, probabilities = topic_model$probabilities, filename = "vis_topic_dist_demo")
visualize_heatmap(topic_model$model, filename = "vis_heat_demo")
visualize_topics(model = topic_model$model, filename = "dist_map_demo")
visualize_hierarchy(model = topic_model$model, hierarchical_topics = NULL, filename = "vis_hiclust_basic_demo", auto_open = FALSE)
# reduced_embeddings_2d and reduced_embeddings_3d in topic_model already computed
visualize_documents(model = topic_model$model, texts = docs, reduced_embeddings = topic_model$reduced_embeddings_2d, filename = "vis_docs_demo")
visualize_documents_2d(model = topic_model$model, texts = docs, reduced_embeddings = topic_model$reduced_embeddings_2d, filename = "vis_docs_demo")
visualize_documents_3d(model = topic_model$model, texts = docs, reduced_embeddings = topic_model$reduced_embeddings_3d, filename = "vis_docs_3d_demo")
# hierarchical_topics = topic_model$model$hierarchical_topics(docs)
visualize_hierarchy(model = topic_model$model, hierarchical_topics = topic_model$hierarchical_topics, filename = "vis_hiclust_demo", auto_open = FALSE)
# To be properly implemented in the train_bertopc_model function
# timestamps <- as.list(df$date)
# timestamps <- lapply(timestamps, function(x) {
#   format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
# })
# topics_over_time  <- topic_model$model$topics_over_time(docs, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)
visualize_topics_over_time(model = topic_model$model, topics_over_time_model = topic_model$topics_over_time, filename = "topics_time_demo")
# # To be properly implemented in the train_bertopc_model function
# classes = as.list(df$genre) # text types
# topics_per_class = topic_model$model$topics_per_class(docs, classes=classes)
visualize_topics_per_class(topic_model$model, topics_per_class = topic_model$topics_per_class, filename = "topic_class_demo")
names(doc_info)
write_csv(doc_info, "doc_inf.csv")
# reduced_embeddings_2d and reduced_embeddings_3d in topic_model already computed
visualize_documents(model = topic_model$model, texts = docs, reduced_embeddings = topic_model$reduced_embeddings_2d, filename = "vis_docs_demo")
library(tidyverse)
library(reticulate)
library(bertopicr)
source("R/train_bertopic_model.R")
# reticulate::use_virtualenv("c:/Users/teodo/Documents/.virtualenvs/rbtopic/", required = TRUE)
reticulate::use_virtualenv("c:/Users/teodo/Documents/R/bertopic/bertopic4r/", required = TRUE)
reticulate::py_config()
reticulate::py_available()
reticulate::py_run_string(code = "import torch
print(torch.cuda.is_available())") # GPU available (TRUE or FALSE)
sample <- "C:/Users/teodo/Documents/bertopic4r/inst/extdata/spiegel_sample.rds"
all_stopwords <- read_lines("C:/Users/teodo/Documents/bertopic4r/inst/extdata/all_stopwords.txt")
df <- read_rds(sample)
# df <- df |> head(1000)
docs <- df |> pull(text_clean)
topic_model <- train_bertopic_model(
docs = docs,
top_n_words = 50L,
embedding_model = "Qwen/Qwen3-Embedding-0.6B",
embedding_show_progress = TRUE,
representation_model = "keybert")
(doc_info <- get_document_info_df(model = topic_model$model, texts = docs))
(topic_info <- get_topic_info_df(topic_model$model))
get_most_representative_docs(doc_info |> rename(probs = Probability), topic_nr = 1, n_docs = 2)
get_topic_df(topic_model$model, topic_number = 1)
doc_info |> write_csv("doc_info.csv")
library(wordcloud2)
source("inst/extdata/wordcloud2a.R")
df_wc <- get_topic_df(topic_model$model, topic_number = 0, top_n = 50)
wordcloud2a(df_wc)
wordcloud2a(df_wc, size = 0.5)
wordcloud2a(df_wc, size = 0.4)
wordcloud2a(df_wc, size = 0.3)
df_wc
dfs_wc <- get_topics_df(model = topic_model)
dfs_wc <- get_topics_df(model = topic_model$model)
dfs_wc
wordcloud2a(dfs_wc, size = 0.3)
wordcloud2a(dfs_wc, size = 0.3, color = as.factor(Topic))
dfs_wc <- get_topics_df(model = topic_model$model)
wordcloud2a(dfs_wc, size = 0.3, color = as.factor(topic_model$topics))
wordcloud2a(dfs_wc, size = 0.3, color = topic_model$topics)
wordcloud2a(dfs_wc, size = 0.3, color = as.factor(dfs_wc$Topic))
dfs_wc <- get_topics_df(model = topic_model$model)
wordcloud2a(dfs_wc, size = 0.3, color = as.factor(dfs_wc$Topic |> filter(dfs_wc > 0)))
wordcloud2a(dfs_wc, size = 0.3, color = as.factor(dfs_wc$Topic |> filter(dfs_wc$Topic > 0)))
mytopics <- dfs_wc |> filter(Topic > 0) |> select(Topic)
dfs_wc <- get_topics_df(model = topic_model$model)
mytopics <- dfs_wc |> filter(Topic > 0) |> select(Topic)
wordcloud2a(dfs_wc, size = 0.3, color = as.factor(mytopics))
mytopics
mytopics <- dfs_wc |> filter(Topic > 0) |> select(Topic) |> pull(Topic)
wordcloud2a(dfs_wc, size = 0.3, color = as.factor(mytopics))
dfs_wc <- get_topics_df(model = topic_model$model)
colorVector = rep(c('red', 'skyblue'), length.out=nrow(dfs_wc$Topic))
colorVector = rep(c('red', 'skyblue'), length.out=nrow(dfs_wc$Word))
colorVector = rep(c('red', 'skyblue'), length.out=nrow(dfs_wc$Score))
colorVector = rep(c('red', 'skyblue'), length.out=nrow(dfs_wc))
colorVector
colorVector = rep(rainbow(34), length.out=nrow(dfs_wc))
wordcloud2a(dfs_wc, size = 0.3, color = colorVector)
wordcloud2a(dfs_wc, size = 0.2, color = colorVector)
colorVector
wordcloud2a(dfs_wc |> filter(Topic == 0), size = 0.2, color = colorVector)
colorVector = rep(rainbow(34, 10), length.out=nrow(dfs_wc))
colorVector = rep(rainbow(34), 10, length.out=nrow(dfs_wc))
wordcloud2a(dfs_wc |> filter(Topic == 0), size = 0.2, color = colorVector)
colorVector = rep(rainbow(34), 10)
wordcloud2a(dfs_wc |> filter(Topic == 0), size = 0.2, color = colorVector)
colorVector = rep(rainbow(34), 10)
wordcloud2a(dfs_wc, size = 0.2, color = colorVector)
colorVector
colorVector = rep(rainbow(34), each = 10)
colorVector
colorVector = rep(rainbow(34), each = 10, length.out=nrow(dfs_wc))
colorVector
wordcloud2a(dfs_wc, size = 0.2, color = colorVector)
dfs_wc <- get_topics_df(model = topic_model$model) |> filter(Topic >= 0)
colorVector = rep(rainbow(34), each = 10, length.out=nrow(dfs_wc))
wordcloud2a(dfs_wc, size = 0.2, color = colorVector)
colorVector = rep(sample(rainbow(34)), each = 10, length.out=nrow(dfs_wc))
colorVector
wordcloud2a(dfs_wc, size = 0.2, color = colorVector)
wordcloud2a(dfs_wc, size = 0.1, color = colorVector)
wordcloud2a(dfs_wc, size = 0.15, color = colorVector)
colorVector = rep(sample(rainbow(34), 10), each = 10, length.out=nrow(dfs_wc))
wordcloud2a(dfs_wc, size = 0.15, color = colorVector)
colorVector = rep(sample(rainbow(34), 2), each = 10, length.out=nrow(dfs_wc))
wordcloud2a(dfs_wc, size = 0.15, color = colorVector)
colorVector = rep(sample(rainbow(34), 34), each = 10, length.out=nrow(dfs_wc))
wordcloud2a(dfs_wc, size = 0.15, color = colorVector)
(topic_info <- get_topic_info_df(topic_model$model))
save(topic_model, "topic_model")
save(topic_model, file = "topic_model")
library(readr)
library(dplyr)
library(wordcloud2)
source("inst/extdata/wordcloud2a.R")
library(reticulate)
library(bertopicr)
source("R/train_bertopic_model.R")
reticulate::use_virtualenv("c:/Users/teodo/Documents/R/bertopic/bertopic4r/", required = TRUE)
reticulate::py_config()
reticulate::py_available()
reticulate::py_run_string(code = "import torch
print(torch.cuda.is_available())") # GPU available (TRUE or FALSE)
sample <- "C:/Users/teodo/Documents/bertopic4r/inst/extdata/spiegel_sample.rds"
all_stopwords <- read_lines("C:/Users/teodo/Documents/bertopic4r/inst/extdata/all_stopwords.txt")
df <- read_rds(sample)
docs <- df |> pull(text_clean)
# save(topic_model, file = "topic_model")
load("topic_model")
(doc_info <- get_document_info_df(model = topic_model$model, texts = docs))
r_to_py(topic_model)
(doc_info <- get_document_info_df(model = topic_model$model, texts = docs))
topic_model$topics
topic_model$model
library(readr)
library(dplyr)
library(wordcloud2)
source("inst/extdata/wordcloud2a.R")
library(reticulate)
library(bertopicr)
source("R/train_bertopic_model.R")
reticulate::use_virtualenv("c:/Users/teodo/Documents/R/bertopic/bertopic4r/", required = TRUE)
reticulate::py_config()
reticulate::py_available()
reticulate::py_run_string(code = "import torch
print(torch.cuda.is_available())") # GPU available (TRUE or FALSE)
sample <- "C:/Users/teodo/Documents/bertopic4r/inst/extdata/spiegel_sample.rds"
all_stopwords <- read_lines("C:/Users/teodo/Documents/bertopic4r/inst/extdata/all_stopwords.txt")
df <- read_rds(sample)
docs <- df |> pull(text_clean)
tictoc::tic()
topic_model <- train_bertopic_model(
docs = docs,
top_n_words = 50L,
embedding_model = "Qwen/Qwen3-Embedding-0.6B",
embedding_show_progress = TRUE,
representation_model = "keybert"
)
tictoc::toc()
# after training
topic_model$model$save("topic_model")  # writes a folder
saveRDS(list(
probabilities = topic_model$probabilities,
topics_over_time = topic_model$topics_over_time,
topics_per_class = topic_model$topics_per_class,
reduced_embeddings_2d = topic_model$reduced_embeddings_2d,
reduced_embeddings_3d = topic_model$reduced_embeddings_3d
), "topic_model_extras.rds")
library(readr)
library(dplyr)
library(wordcloud2)
source("inst/extdata/wordcloud2a.R")
library(reticulate)
library(bertopicr)
source("R/train_bertopic_model.R")
reticulate::use_virtualenv("c:/Users/teodo/Documents/R/bertopic/bertopic4r/", required = TRUE)
reticulate::py_config()
reticulate::py_available()
reticulate::py_run_string(code = "import torch
print(torch.cuda.is_available())") # GPU available (TRUE or FALSE)
sample <- "C:/Users/teodo/Documents/bertopic4r/inst/extdata/spiegel_sample.rds"
all_stopwords <- read_lines("C:/Users/teodo/Documents/bertopic4r/inst/extdata/all_stopwords.txt")
df <- read_rds(sample)
docs <- df |> pull(text_clean)
model <- bertopic$BERTopic$load("topic_model")
# new session
bertopic <- reticulate::import("bertopic")
model <- bertopic$BERTopic$load("topic_model")
doc_info <- get_document_info_df(model = model, texts = docs)
doc_info
extras <- readRDS("topic_model_extras.rds")
extras
# To be properly implemented in the train_bertopc_model function
# timestamps <- as.list(df$date)
# timestamps <- lapply(timestamps, function(x) {
#   format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
# })
# topics_over_time  <- topic_model$model$topics_over_time(docs, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)
visualize_topics_over_time(model = topic_model$model, topics_over_time_model = topic_model$topics_over_time, filename = "topics_time_demo")
model$visualize_documents(docs)
visualize_documents_2d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_2d)
visualize_documents_3d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_3d)
visualize_topics_over_time(model = model, topics_over_time_model = extras$topic_model$topics_over_time)
extras$topic_model$topics_over_time
visualize_topics_over_time(model = model, topics_over_time_model = extras$topics_over_time)
visualize_topics_per_class(model, extras$topics_per_class)
visualize_barchart(model = model)
visualize_distribution(model, text_id = 1, probabilities = extras$probabilities)
visualize_heatmap(model)
visualize_topics(model)
visualize_hierarchy(model, hierarchical_topics = extras$hierarchical_topics)
(doc_info <- get_document_info_df(model = model, texts = docs))
(topic_info <- get_topic_info_df(model))
get_most_representative_docs(doc_info |> rename(probs = Probability), topic_nr = 1, n_docs = 2)
get_topic_df(model, topic_number = 1)
knitr::opts_chunk$set(eval = FALSE)
library(reticulate)
library(bertopicr)
library(readr)
library(dplyr)
reticulate::use_virtualenv("c:/Users/teodo/Documents/R/bertopic/bertopic4r/", required = TRUE)
reticulate::py_config()
reticulate::py_available()
reticulate::py_run_string(code = "import torch
print(torch.cuda.is_available())") # GPU available (TRUE or FALSE)
source("../inst/extdata/wordcloud2a.R")
source("../R/train_bertopic_model.R")
source("../R/load_bertopic_model.R")
source("../R/save_bertopic_model.R")
loaded <- load_bertopic_model("topic_model") # set the location of the model!
model <- loaded$model
extras <- loaded$extras
sample_path <- system.file("extdata", "spiegel_sample.rds", package = "bertopicr")
df <- read_rds(sample_path)
docs <- df |> pull(text_clean)
doc_info <- get_document_info_df(model = model, texts = docs)
topic_info <- get_topic_info_df(model = model)
topics_df <- get_topics_df(model = model)
visualize_barchart(model = model, filename = "barchart_demo")
visualize_distribution(
model = model,
text_id = 1,
probabilities = extras$probabilities,
filename = "vis_topic_dist_demo"
)
visualize_heatmap(model = model, filename = "vis_heat_demo")
visualize_topics(model = model, filename = "dist_map_demo")
visualize_documents(model = model, docs, reduced_embeddings = extras$reduced_embeddings_2d)
visualize_documents_2d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_2d)
visualize_documents_3d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_3d)
visualize_topics_over_time(model = model, topics_over_time_model = extras$topics_over_time)
visualize_topics_per_class(model, extras$topics_per_class, auto_open = FALSE)
visualize_topics_over_time(model = model, topics_over_time_model = extras$topics_over_time)
visualize_topics_per_class(model, extras$topics_per_class, auto_open = FALSE)
topic_model <- train_bertopic_model(
docs = docs,
top_n_words = 50L, # set integer numbger of top words
embedding_model = "Qwen/Qwen3-Embedding-0.6B", # choose your (multilingual) model from huggingface.co
embedding_show_progress = TRUE,
timestamps = df$date, # set this to NULL if not applicable with your data
classes = df$genre, # set this to NULL if not applicable with your data
representation_model = "keybert" # keyword generation for each topic
)
save_bertopic_model(topic_model, "topic_model")
loaded <- load_bertopic_model("topic_model") # set the location of the model!
model <- loaded$model
extras <- loaded$extras
sample_path <- system.file("extdata", "spiegel_sample.rds", package = "bertopicr")
df <- read_rds(sample_path)
docs <- df |> pull(text_clean)
doc_info <- get_document_info_df(model = model, texts = docs)
topic_info <- get_topic_info_df(model = model)
topics_df <- get_topics_df(model = model)
visualize_barchart(model = model, filename = "barchart_demo")
visualize_distribution(
model = model,
text_id = 1,
probabilities = extras$probabilities,
filename = "vis_topic_dist_demo"
)
visualize_heatmap(model = model, filename = "vis_heat_demo")
visualize_topics(model = model, filename = "dist_map_demo")
visualize_documents(model = model, docs, reduced_embeddings = extras$reduced_embeddings_2d)
visualize_documents_2d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_2d)
visualize_documents_3d(model = model, docs, reduced_embeddings = extras$reduced_embeddings_3d)
visualize_topics_over_time(model = model, topics_over_time_model = extras$topics_over_time)
visualize_topics_per_class(model, extras$topics_per_class, auto_open = FALSE)
library(readr)
library(dplyr)
library(bertopicr)
sample_path <- system.file("extdata", "spiegel_sample.rds", package = "bertopicr")
df <- read_rds(sample_path)
docs <- df |> pull(text_clean)
stopword_path <- system.file("extdata", "all_stopwords.txt", package = "bertopicr")
all_stopwords <- read_lines(stopword_path)
all_stopwords
df <- read_rds(sample)
df <- read_rds(sample_path)
source("R/wordcloud2a.R")
source("../R/wordcloud2a.R")
source("../../R/wordcloud2a.R")
source("extdata/wordcloud2a.R")
source("inst/extdata/wordcloud2a.R")
setup_python_environment <- function(
envname = "rbtopic",
python_path = NULL,
method = "virtualenv",  # or "conda
python_version = NULL,
upgrade = TRUE,
extra_packages = NULL
)
setup_python_environment <- function(
envname = "rbtopic",
python_path = NULL,
method = "virtualenv",  # or "conda
python_version = NULL,
upgrade = TRUE,
extra_packages = NULL
)
library(reticulate)
setup_python_environment <- function(
envname = "rbtopic",
python_path = NULL,
method = "virtualenv",  # or "conda
python_version = NULL,
upgrade = TRUE,
extra_packages = NULL
)
source("R/setup_python_environment.R")
source("R/setup_python_environment.R")
setup_python_environment <- function(
envname = "rbtopic",
python_path = NULL,
method = "virtualenv",  # or "conda
python_version = NULL,
upgrade = TRUE,
extra_packages = NULL
)
library(reticulate)
source("R/setup_python_environment.R")
devtools::document()
devtools::check()
devtools::document()
devtools::check()
pkgdown::build_site()
devtools::document()
devtools::check()
knitr::opts_chunk$set(eval = FALSE)
library(reticulate)
library(bertopicr)
library(readr)
library(dplyr)
# macOS: if reticulate fails to load Python libraries, run once per session.
bertopicr::configure_macos_homebrew_zlib()
source("R/setup_python_environment.R")
source("../R/setup_python_environment.R")
# macOS: if reticulate fails to load Python libraries, run once per session.
bertopicr::configure_macos_homebrew_zlib()
configure_macos_homebrew_zlib()
devtools::document()
devtools::load_all()
devtools::document()
devtools::check()
devtools::check(args = "--as-cran")
R CMD check --as-cran
rcmdcheck::rcmdcheck(args = "--as-cran")
rcmdcheck::rcmdcheck(args = "--as-cran")
devtools::check(args = "--as-cran")
devtools::document()
devtools::check()
Sys.setenv(BERTOPICR_VENV = "C:/path/to/your/venv", NOT_CRAN = "true")
Sys.setenv(BERTOPICR_VENV = "C:/path/to/your/venv", NOT_CRAN = "true")
Sys.setenv(BERTOPICR_VENV = "C:/path/to/your/venv", NOT_CRAN = "true")
Sys.getenv(BERTOPICR_VENV)
Sys.getenv(c("BERTOPICR_VENV", "NOT_CRAN"))
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
# 1. Define the libraries you need
required_modules <- c("bertopic", "umap", "hdbscan", "sklearn", "numpy", "plotly", "datetime", "sentence_transformers", "openai", "ollama")
# Optional: point reticulate at a user-specified virtualenv
venv <- Sys.getenv("BERTOPICR_VENV")
if (nzchar(venv)) {
venv_cfg <- file.path(venv, "pyvenv.cfg")
if (file.exists(venv_cfg)) {
reticulate::use_virtualenv(venv, required = TRUE)
} else {
message("Warning: BERTOPICR_VENV does not point to a valid virtualenv: ", venv)
}
}
# 2. Check if they are installed
python_ready <- tryCatch({
# Attempt to initialize python and check modules
py_available(initialize = TRUE) &&
all(vapply(required_modules, py_module_available, logical(1)))
}, error = function(e) FALSE)
# 3. Only evaluate chunks when Python is ready and NOT_CRAN is set
run_chunks <- python_ready && identical(Sys.getenv("NOT_CRAN"), "true")
knitr::opts_chunk$set(eval = run_chunks)
if (!python_ready) {
message("Warning: Required Python modules (bertopic, umap-learn) not found. Vignette code will not run.")
} else if (!identical(Sys.getenv("NOT_CRAN"), "true")) {
message("Note: Set NOT_CRAN=true to run Python-dependent chunks locally.")
}
Sys.setenv(
BERTOPICR_VENV = "C:/path/to/your/venv",
NOT_CRAN = "true"
)
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
# Replace the path below with the path of your Python environment
# Then uncomment the command below:
# Tip: BERTOPICR_VENV should be the folder that contains `pyvenv.cfg`.
Sys.setenv(
BERTOPICR_VENV = "C:/Users/teodo/Documents/R/bertopic/bertopic4r",
NOT_CRAN = "true"
)
# 1. Define the libraries you need
required_modules <- c("bertopic", "umap", "hdbscan", "sklearn", "numpy", "plotly", "datetime", "sentence_transformers", "openai", "ollama")
# Optional: point reticulate at a user-specified virtualenv
venv <- Sys.getenv("BERTOPICR_VENV")
if (nzchar(venv)) {
venv_cfg <- file.path(venv, "pyvenv.cfg")
if (file.exists(venv_cfg)) {
reticulate::use_virtualenv(venv, required = TRUE)
} else {
message("Warning: BERTOPICR_VENV does not point to a valid virtualenv: ", venv)
}
}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
# Replace the path below with the path of your Python environment
# Then uncomment the command below:
# Tip: BERTOPICR_VENV should be the folder that contains `pyvenv.cfg`.
Sys.setenv(
BERTOPICR_VENV = "C:/Users/teodo/Documents/R/bertopic/bertopic4r",
NOT_CRAN = "true"
)
# 1. Define the libraries you need
required_modules <- c("bertopic", "umap", "hdbscan", "sklearn", "numpy", "plotly", "datetime", "sentence_transformers", "openai", "ollama")
# Optional: point reticulate at a user-specified virtualenv
venv <- Sys.getenv("BERTOPICR_VENV")
if (nzchar(venv)) {
venv_cfg <- file.path(venv, "pyvenv.cfg")
if (file.exists(venv_cfg)) {
reticulate::use_virtualenv(venv, required = TRUE)
} else {
message("Warning: BERTOPICR_VENV does not point to a valid virtualenv: ", venv)
}
}
# 2. Check if they are installed
python_ready <- tryCatch({
# Attempt to initialize python and check modules
py_available(initialize = TRUE) &&
all(vapply(required_modules, py_module_available, logical(1)))
}, error = function(e) FALSE)
# 3. Only evaluate chunks when Python is ready and NOT_CRAN is set
run_chunks <- python_ready && identical(Sys.getenv("NOT_CRAN"), "true")
knitr::opts_chunk$set(eval = run_chunks)
if (!python_ready) {
message("Warning: Required Python modules (bertopic, umap-learn) not found. Vignette code will not run.")
} else if (!identical(Sys.getenv("NOT_CRAN"), "true")) {
message("Note: Set NOT_CRAN=true to run Python-dependent chunks locally.")
}
